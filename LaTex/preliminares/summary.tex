% !TeX root = ../tfg.tex
% !TeX encoding = utf8
%
%*******************************************************
% Summary
%*******************************************************

\selectlanguage{english}
\chapter{Summary}

\noindent\textbf{Keywords:} Machine Learning, Deep Learning, Artificial Intelligence, Bias-Variance, Non-Linear Approximation, Computer Vision

\

Machine learning, particularly deep learning, has become a fundamental tool in many fields, playing a decisive role in complementing and even replacing human work in various contexts. However, the training of increasingly complex models has recently revealed unexpected behaviors in their performance, among which the phenomenon known as \emph{Deep Double Descent} stands out. This behavior challenges the classical wisdom of learning by showing that the relationship between model complexity and performance does not conform to the traditional curves predicted by conventional theory.\newline

In the classical machine learning approach, increasing the capacity of a model improves its ability to fit the data, but eventually it leads to overfitting, at which point his performance begins to deteriorate. In contrast, recent findings have shown that as the model continues to increase its capacity beyond that overfitting point, its performance begins to improve again, creating a second descent that breaks with classical intuition. This behavior reveals a significant gap between theoretical knowledge and the empirical results currently observed, reminding us that the field of artificial intelligence is constantly evolving, and that advances in this discipline do not occur in parallel between theory and practice.\newline

In this Final Degree Project, we will focus on exposing the computational and mathematical principles essential to understand \emph{Deep Double Descent}, starting from the classical framework of bias-variance tradeoff, for which it will be necessary to resort to notions of probability theory and statistics. Likewise, concepts related to matrix algebra, which are essential to understand it in depth, will be reviewed. Thus, the main objective of the work is to present the knowledge available to date to explain this event, complementing it with the performance of various experiments that empirically support the theoretical information, in order to discuss some of the open questions raised by the current literature.\newline

On the other hand, the theory of nonlinear approximation will be developed, given its prior and significant relationship with \emph{Deep Double Descent}, highlighting the different analogies they share. With all this, we seek to contribute to the construction of a bridge to reconcile classical wisdom with modern discoveries, thus favoring a more unified understanding of contemporary machine learning.\newline

\clearpage
\thispagestyle{empty}
\mbox{}
\newpage
% Al finalizar el resumen en inglés, volvemos a seleccionar el idioma español para el documento
\selectlanguage{spanish} 
\endinput
