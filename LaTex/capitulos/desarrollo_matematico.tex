% !TeX root = ../tfg.tex
% !TeX encoding = utf8

\chapter{Análisis teórico del Deep Double Descent}\label{ch:analisis-teorico-ddd}

En este capítulo, nos vamos a encargar de abordar, de manera teórica, el doble descenso profundo. En primer lugar, lo definiremos con la mayor precisión posible, ofreciendo una intuición clara a partir de un problema de regresión. A continuación, se presentarán los principales desarrollos presentes en la literatura científica, así como algunos avances recientes en el tema. Para concluir el capítulo, se abordará la teoría de la aproximación no lineal, ya que, a priori, presenta ciertas analogías con este fenómeno.\newline

\section{Planteamiento teórico}\label{sec:planteamiento-teorico}

Siguiendo los resultados expuestos en la Sección~\ref{sec:subsec-underfitting-y-overfitting}, la sabiduría clásica adopta una visión en la cual sostiene que los modelos más grandes tienden a ser peores, ya que su capacidad de generalización empeora. En la práctica moderna, especialmente ahora en la era del deep learning, es cada vez más común el uso de modelos de gran tamaño con suficientes parámetros para reducir el error de entrenamiento casi a cero. A pesar de ajustarse casi perfectamente a los datos, estos modelos logran generalizar sorprendentemente bien e incluso superar en rendimiento a modelos más simples.\newline

De esta manera, se ha comprobado que, más allá de cierto umbral, el aumento de la capacidad de los modelos resulta beneficioso, ya que no conduce al sobreajuste y, en realidad, disminuye nuevamente el error de generalización. A esta nueva zona de funcionamiento de los modelos la denotaremos como \emph{régimen moderno} o zona sobreparametrizada, mientras que la región previa al umbral, en la que se produce la tradicional curva con forma de ``U'', la denominaremos como \textit{régimen clásico} o zona infraparametrizada.\newline

De cara a formalizar el fenómeno del doble descenso profundo y unificar la sabiduría clásica con la práctica moderna, introducimos una nueva medida de capacidad del modelo, propuesta por Nakkiran et al.\ en~\cite{Nakkiran2019}.

\begin{definicion}[Complejidad efectiva del modelo]
    La complejidad efectiva del modelo (EMC, por sus siglas en inglés) de un algoritmo de aprendizaje $\mathcal{A}$, con respecto a la distribución de probabilidad conjunta $P[X, Y]$ de los datos del conjunto de entrenamiento $\mathcal{D}$, es el máximo número de ejemplos de entrenamiento $n$ en el que $\mathcal{A}$ obtiene, de media, un error de entrenamiento muy próximo a cero. Es decir, dado $\epsilon > 0$:

    \[
        EMC_{P, \epsilon}(\mathcal{A}) = \max\{ n \in \mathbb{N} \; | \; \mathbb{E}[L(g)] \leq \epsilon \}
    \]  

    donde $L(g)$ hace referencia a la función de pérdida de la función candidata $g \in \mathcal{H}$.\newline
\end{definicion}

Una vez definida la noción de complejidad efectiva del modelo, se expone uno de los resultados principales de este trabajo.

\begin{hipotesis}\label{hipotesis-general-double-descent}
    Para cualquier distribución de datos $P[X, Y]$, algoritmo de aprendizaje basado en redes neuronales $\mathcal{A}$ (véase Subsección~\ref{subsubsec:aprendizaje-red-neuronal}) y un pequeño $\epsilon > 0$, si consideramos la tarea de predecir etiquetas basadas en $n$ muestras aleatorias e independientes de $P[X, Y]$, entonces:

    \begin{itemize}
        \item \textbf{Región infraparametrizada.} Si $EMC_{P, \epsilon}(\mathcal{A})$ es suficientemente menor que $n$, entonces cualquier perturbación de $\mathcal{A}$ que aumente su complejidad efectiva, disminuirá su error de generalización.
        \item \textbf{Región crítica.} Si $EMC_{P, \epsilon}(\mathcal{A}) \approx n$, entonces cualquier perturbación de $\mathcal{A}$ que aumente su complejidad efectiva puede aumentar o disminuir su error de generalización.
        \item \textbf{Región sobreparametrizada.} Si $EMC_{P, \epsilon}(\mathcal{A})$ es suficientemente mayor que $n$, entonces cualquier perturbación de $\mathcal{A}$ que aumente su complejidad efectiva, disminuirá su error de generalización.\newline
    \end{itemize}
\end{hipotesis}

Esta hipótesis es informal en varios sentidos. En primer lugar, no disponemos de una forma precisa de elegir el parámetro $\epsilon$, ni de una especificación formal para ``suficientemente pequeño'' y ``suficientemente grande''. La hipótesis sugiere que hay un intervalo crítico alrededor del umbral de interpolación $(EMC_{P, \epsilon}(\mathcal{A}) = n)$ de manera que, tanto por debajo como por encima de dicho intervalo, el aumento de la complejidad beneficia el rendimiento del modelo, mientras que en el interior de este intervalo el comportamiento es incierto, pudiendo mejorar o empeorar. Además, la amplitud de dicho intervalo depende tanto de la distribución de los datos del conjunto de entrenamiento $\mathcal{D}$ como del algoritmo de aprendizaje $\mathcal{A}$ utilizado.\newline

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\linewidth]{img/planteamiento-teorico-dd.png}
    \caption[Ejemplo de doble descenso con las distintas zonas de parametrización.]{Ejemplo de doble descenso con las distintas zonas de parametrización. Se puede observar la tradicional forma de ``U'' en la zona clásica, seguida de la zona crítica, donde el error de generalización es incierto, ya que puede aumentar o disminuir, y, finalmente, la zona moderna, donde el error de generalización es incluso menor que el obtenido en la zona clásica. Imagen original del autor.}\label{fig:planteamiento-teorico-dd.png}
\end{figure}

Por otra parte, la Hipótesis~\ref{hipotesis-general-double-descent} nos ayuda a unificar la sabiduría clásica con los resultados modernos en una curva de aprendizaje que abarca ambos mundos. Hasta el umbral de interpolación, la curva sigue la tradicional forma de ``U'', cuyo mínimo se alcanza en el \textit{sweet spot} (véase Figura~\ref{fig:biasvarianceunderoverfitting}) mientras que, aumentando la capacidad del modelo hasta alcanzar un error de entrenamiento muy próximo a cero, y tras superar la zona crítica donde se encuentra el umbral de interpolación, nos indica que el error de generalización comienza nuevamente a disminuir, pudiendo lograr un error menor que el obtenido en el \textit{sweet spot}.\newline

Este comportamiento, en el que la curva del error de generalización exhibe dos descensos, puede apreciarse en la Figura~\ref{fig:planteamiento-teorico-dd.png} y lo denotaremos por \textbf{doble descenso profundo}, o simplemente \textbf{doble descenso}.\newline


De esta manera, estamos separando las distintas zonas de funcionamiento de un modelo en función, en cierta medida, de su conjunto de hipótesis $\mathcal{H}$. En la zona clásica, por lo general, no existe ninguna hipótesis que minimice completamente el riesgo real, es decir, no existe $g \in \mathcal{H}$ tal que $L(g) = 0$. Por el contrario, en la zona moderna, conforme aumenta la capacidad del modelo, aparece un conjunto cada vez más amplio $S \subset \mathcal{H}$ que interpola a la perfección los datos de entrenamiento. Este conjunto se define como:  

\[
    S = \{ g \in \mathcal{H} \mid L(g) = 0 \}.
\]

En consecuencia, el verdadero problema radica en comprender por qué el algoritmo de aprendizaje $\mathcal{A}$, en la zona sobreparametrizada, tiende a seleccionar ``buenas'' soluciones dentro del conjunto $S$, es decir, aquellas que logran una buena generalización. Es importante destacar que esta cuestión no se puede responder únicamente a partir de los datos del conjunto de entrenamiento $\mathcal{D}$, ya que cualquier hipótesis de $S$ se ajusta perfectamente a esos datos. Por tanto, la clave para comprender el fenómeno vendrá dada por el sesgo inductivo que presentan algunos de los algoritmos de optimización más comunes, los cuales favorecen ciertas soluciones dentro de $S$.\newline

\subsection{Análisis intuitivo en un problema de mínimos cuadrados}\label{subsec:analisis-intuitivo-minimos-cuadrados}

De cara a ofrecer una primera intuición sencilla sobre la ocurrencia del doble descenso, consideramos un problema de regresión lineal (basado en~\cite{Schaeffer2023}) que resolveremos utilizando el método de mínimos cuadradados ordinarios (OLS), cuya solución de norma mínima viene dada por la pseudoinversa, como vimos en la Sección~\ref{sec:svd-pseudoinversa}. Además, para comprender dónde y cómo se produce este suceso, estudiaremos las dos zonas de parametrización del modelo de regresión lineal, analizando los diferentes errores que se producen en cada una de ellas.\newline

Supongamos que nos enfrentamos ante un problema de regresión lineal en el que disponemos de $N$ ejemplos, donde cada ejemplo está formado por los respectivos datos de entrada y su correspondiente etiqueta. Sea $D$ la dimensión de los puntos de entrenamiento y $P$ el número de parámetros a ajustar para realizar la regresión lineal. De esta manera, nuestro conjunto de entrenamiento $\mathcal{D}$ está formado por $N$ pares de la forma $(x,y)$, donde $x \in \mathbb{R}^{D}$ representa un punto en el plano $D$-dimensional e $y \in \mathbb{R}$ la variable objetivo.\newline

Para resolver el problema de regresión lineal, debemos abordar el clásico problema de minimización de mínimos cuadrados, formulado como sigue

\begin{equation}\label{eq:minimos-cuadrados1}
    \arg\min_{w} \frac{1}{N}\sum_{n=1}^{N}\| x_n \cdot w - y_n \|^{2} = \arg\min_{w}\| Xw - Y \|^{2}
\end{equation}

donde cada par $(x, y)$ representa un ejemplo de entrenamiento y $w$ indica el vector de pesos o parámetros que queremos aprender de cara a realizar la aproximación. Además, $X \in \mathcal{M}_{N \times D}(\mathbb{R})$ y $Y\in \mathcal{M}_{N \times 1}(\mathbb{R})$ representan, respectivamente, los puntos de entrenamiento y su correspondiente salida en forma matricial.\newline

En la zona infraparametrizada, es decir, cuando el número de ejemplos de entrenamiento es mayor que el número de parámetros a ajustar $(P < N)$, la solución al problema de minimización~\eqref{eq:minimos-cuadrados1}, ofrecida por la pseudoinversa de $X$, viene dada por ${(X^{T}X)}^{-1}X^{T}$. De este modo, el vector de pesos $w$ puede ser expresado como:

\begin{equation}
    w_{under} = {(X^{T}X)}^{-1}X^{T}Y.
\end{equation}\newline

Por otra parte, en la zona sobreparametrizada, es decir, cuando el número de parámetros es mayor que el número de ejemplos de entrenamiento $(N < P)$, no se puede resolver el problema de minimización~\eqref{eq:minimos-cuadrados1} de manera convencional, dado que estaría mal planteado, debido al hecho de que existirían múltiples soluciones, ya que habría menos restricciones que parámetros. Por tanto, debemos de seleccionar un problema de optimización distinto, que también se encuentre sujeto a las restricciones impuestas por los ejemplos de entrenamiento. En este contexto, elegimos el problema más simple que se podría imaginar, dado por:

\begin{equation}\label{eq:minimos-cuadrados2}
    \arg\min_{w}\| w \|^{2}, \quad \text{sujeto a} \; \forall n \in \{1, \ldots, N\} \quad x_n \cdot w = y_n.
\end{equation}\newline

El problema de optimización~\eqref{eq:minimos-cuadrados2} busca el vector de parámetros con menor norma que satisface $x_n \cdot w = y_n$ para todos los ejemplos de entrenamiento. La solución a este problema de optimización también se obtiene mediante la pseudoinversa, pero en este caso, la pseudoinversa se calcula conociendo que la matriz $X$ tiene rango completo en esta región y cuenta con filas linealmente independientes. Así, la pseudoinversa viene dada por $X^{T}{(XX^{T})}^{-1}$ (véase~\ref{sec:svd-pseudoinversa}). De esta forma, el vector de pesos $w$ puede ser expresado como:

\begin{equation}
    w_{over} = X^{T}{(XX^{T})}^{-1}Y.
\end{equation}\newline

Por tanto, una vez que se ha obtenido el vector de pesos, el modelo realizará las siguientes predicciones para un determinado punto de prueba $x_{\text{test}}$, dependiendo de la zona de parametrización en la que se encuentre:

\begin{equation}
    y_{test, under} = x_{\text{test}} \cdot w_{\text{under}} = x_{\text{test}} \cdot {(X^{T}X)}^{-1}X^{T}Y.
\end{equation}

\begin{equation}
    y_{test, over} = x_{\text{test}} \cdot w_{\text{over}} = x_{\text{test}} \cdot X^{T}{(XX^{T})}^{-1}Y.
\end{equation}\newline

No obstante y llegados a este punto, las diferencias al predecir un punto de prueba en ambas zonas de parametrización no parecen ser del todo claras. Para identificar estas diferencias con mayor precisión, reescribiremos nuestras predicciones de la siguiente forma:

\[
    y_n = x_n \cdot w^{*} + e_n
\]

donde $w^{*} \in \mathbb{R}^{P} = \mathbb{R}^{D}$ representa el vector de parámetros ideal que realmente minimiza el error cuadrático medio y $e_n$ representa un término de error adicional presente en la propia naturaleza de los datos, es decir, un residuo que el modelo no puede capturar. Equivalentemente, en forma matricial, podemos escribir:

\[
    Y = X \cdot w^{*} + E \quad \text{con } E \in \mathbb{R}^{N \times 1}.
\]\newline

Usando esta notación, podemos reformular las predicciones que realiza el modelo. De esta manera, para el caso de la zona infraparametrizada nos queda:

\begin{align}
    y_{test, under} &= x_{test} \cdot {(X^T X)}^{-1} X^T Y \\
    &= x_{test} \cdot {(X^T X)}^{-1} X^T (X w^{*} + E) \\
    &= x_{test} \cdot {(X^T X)}^{-1} X^T X w^{*} + x_{test} \cdot {(X^T X)}^{-1} X^T E \\
    &= \underbrace{x_{test} \cdot w^{*}}_{\overset{def}{=} y^*_{test}} + x_{test} \cdot {(X^T X)}^{-1} X^T E
\end{align}

\begin{equation}
    y_{test, under} - y^{*}_{test} = x_{test} \cdot {(X^T X)}^{-1} X^T E.
\end{equation}\newline

Por otra parte, para el caso de la zona sobreparametrizada, los cálculos serían los siguientes:

\begin{align}
    y_{test, over} &= x_{test} \cdot X^{T}{(XX^{T})}^{-1}Y \\
    &= x_{test} \cdot X^{T}{(XX^{T})}^{-1}(X w^{*} + E) \\
    &= x_{test} \cdot X^{T}{(XX^{T})}^{-1}X w^{*} + x_{test}X^{T}{(XX^{T})}^{-1}E \\
    y_{test, over} - \underbrace{x_{test} \cdot w^{*}}_{\overset{def}{=} y^*_{test}} &= x_{test} \cdot X^{T}{(XX^{T})}^{-1}X w^{*} - x_{test} \cdot I_{D} w^{*} + x_{test} \cdot {(X^{T}X)}^{-1}X^{T}E
\end{align}

\begin{equation}
    y_{test, over} - y^{*}_{test} = x_{test} \cdot (X^{T}{(XX^{T})}^{-1}X - I_{D}) w^{*} + x_{test} \cdot {(X^{T}X)}^{-1}X^{T}E.
\end{equation}\newline


Las ecuaciones obtenidas son importantes, aunque, a simple vista, no nos proporcionan información significativa. De cara a extraer intuiciones más claras, vamos a reemplazar la matriz $X$ por su descomposición en valores singulares (véase Sección~\ref{sec:svd-pseudoinversa}). De este modo, definimos $X = U \Sigma V^{T}$, $R = rang(X)$ y $\sigma_1 > \cdots > \sigma_R > 0$ como los valores singulares no nulos de $X$. Dado que $E \in \mathbb{R}^{N \times 1}$, podemos descomponer el error de predicción de la siguiente manera:

\begin{itemize}
    \item En la zona infraparametrizada:
        \[
            y_{test, under} - y_{test}^{*} = x_{test} \cdot V \Sigma^{\dagger} U^{T} E = \sum_{r=1}^{R}\frac{1}{\sigma_r}(x_{test} \cdot v_r)(u_r \cdot E)
        \]

        donde se ha utilizado que $V \Sigma^{\dagger} U^{T}$ corresponde a la descomposición en valores singulares de la pseudoinversa de $X$, dada por $X^{\dagger} = {(X^T X)}^{-1} X^T$.

    \item De manera análoga a la zona anterior, en la zona sobreparametrizada encontramos: 
        \[
            \begin{aligned}
                y_{test, over} - y_{test}^{*} &= x_{test} \cdot (X^{T}{(XX^{T})}^{-1}X - I_{D}) w^{*} 
                + x_{test} \cdot V \Sigma^{\dagger} U^{T} E \\
                &= x_{test} \cdot (X^{T}{(XX^{T})}^{-1}X - I_{D}) w^{*} 
                + \sum_{r=1}^{R}\frac{1}{\sigma_r}(x_{test} \cdot v_r)(u_r \cdot E)
            \end{aligned}
        \]

        donde se ha utilizado que $V \Sigma^{\dagger} U^{T}$ corresponde a la descomposición en valores singulares de la pseudoinversa de $X$, dada por $X^{\dagger} = X^{T}{(XX^{T})}^{-1}$.\newline
\end{itemize}

LLegados a este punto, podemos analizar con mayor claridad las diferencias entre ambas predicciones. La zona sobreparametrizada incluye el término adicional $x_{test} \cdot (X^{T}{(XX^{T})}^{-1}X - I_{D}) w^{*}$, que viene a ser una proyección del punto de prueba en el espacio de características. Recordemos que, en la zona sobreparametrizada, hay más parámetros que ejemplos de entrenamiento, por lo que, para $N$ ejemplos de entrenamiento en $D = P$ dimensiones, el modelo solo puede captar fluctuaciones de los datos en $N$ dimensiones, pero no tiene visibilidad para captar las fluctuaciones en las restantes $P - N$ dimensiones. Esto provoca la pérdida de información sobre la relación lineal óptima del vector de pesos $w^{*}$, lo que a su vez aumenta el error de predicción. Este término es el asociado al sesgo (véase Sección~\ref{sec:capitulo-bias-variance-tradeoff}), pues, al encontrarnos en la zona sobreparametrizada, disponemos de gran cantidad de hipótesis ligadas a la falta de suficientes restricciones en comparación con el número de parámetros. Esto favorece que la hipótesis ideal se encuentre dentro de ese conjunto de posibles soluciones, lo cual, a su vez, facilita que este término sea cercano a cero.\newline

El otro término, $\sum_{r=1}^{R}\frac{1}{\sigma_r}(x_{test} \cdot v_r)(u_r \cdot E)$, representa la influencia de cada componente singular y es el causante del doble descenso en una regresión lineal resuelta mediante el método OLS. Este término se conoce como varianza y refleja el impacto de modelar las fluctuaciones de los datos en las direcciones singulares y si dicho balanceo está correlacionado con los objetivos de regresión.\newline

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{img/marchenku-pastor.png}
    \caption[Ejemplos de distribuciones de Marchenko-Pastur~\cite{Charles2018}.]{Ejemplos de distribuciones de Marchenko-Pastur~\cite{Charles2018} para distintos valores de la relación de aspecto de la matriz ($Q = \frac{N}{P}$) y varianza fija. Cuando $Q = 1$, la mayoría de valores propios se acumulan alrededor del cero. A medida que $Q$ aumenta, los valores propios mínimos se alejan progresivamente de cero.}\label{fig:marchenkopastur}
\end{figure}

Por tanto, esos tres términos, multiplicados, determinan cuánto contribuye la $r$-ésima componente singular al error de predicción. Además, el ``pico'' del primer ascenso ocurre cerca del umbral de interpolación, debido a que el menor valor singular no nulo suele alcanzar su valor más bajo en dicho umbral, basado en la distribución de Marchenko-Pastur~\cite{Marchenko1967}.\newline

A grandes rasgos, esta distribución describe el comportamiento de los valores propios de matrices aleatorias y se aplica cuando se tiene una matriz aleatoria cuyas entradas son independientes e idénticamente distribuidas, como es el caso de la matriz $X \in \mathcal{M}_{N \times D}(\mathbb{R})$. En particular, la distribución de Marchenko-Pastur describe los valores propios de la matriz de covarianza $XX^{T}$, que a su vez determinan la varianza explicada por las componentes principales del modelo. De este modo, cuando la relación $\frac{N}{P}$ se aproxima a $1$, los valores propios más pequeños se acercan a $0$ (véase Figura~\ref{fig:marchenkopastur}), provocando que los valores singulares también sean casi nulos. Esto ocurre cerca del umbral de interpolación, donde $N = P = D$, y para evitar que el $N$-ésimo dato añada un valor singular pequeño no nulo a los datos de entrenamiento, deben cumplirse dos condiciones: 

\begin{itemize}
    \item Debe haber una dimensión en la que ninguno de los datos de entrenamiento anteriores haya variado significativamente.
    \item El $N$-ésimo dato debe presentar una variación considerable en esa dimensión.\newline
\end{itemize}

Sin embargo, este escenario es muy poco probable debido a que, en la mayoría de los casos, los datos de entrenamiento se muestrean de manera independiente a partir de variables aleatorias que son i.i.d., lo que resulta en una distribución aleatoria de los datos en todas las dimensiones. Al superar este umbral de interpolación, la varianza explicada por cada dimensión de las covariables se hace más evidente y los valores singulares no nulos más pequeños se alejan de cero, lo que reduce nuevamente el término de varianza en las predicciones, pudiendo provocar el doble descenso.\newline

\section{Sesgo inductivo del descenso de gradiente}\label{sec:sesgo-implicito-descenso-gradiente}

En esta sección, de cara a profundizar en la intuición obtenida en la sección anterior, vamos a analizar la influencia de utilizar el descenso de gradiente como método de optimización en problemas de regresión y clasificación, dado que es uno de los métodos más utilizados para entrenar redes neuronales.\newline

\subsection{Problema de regresión}\label{subsec:problema-regresion}
En primer lugar, nos centraremos en abordar un problema de regresión utilizando el descenso de gradiente (basándonos en~\cite{Lafon2024}), en lugar de utilizar la solución que nos ofrece el problema de mínimos cuadrados asociado. Para ello, vamos a considerar nuevamente un problema de mínimos cuadrados bajo los mismos supuestos de la sección anterior. Por tanto, suponemos que nuestro conjunto de entrenamiento $\mathcal{D}$ está formado por $N$-pares de la forma $(x, y)$, donde $x \in \mathbb{R}^{D}$ y $y \in \mathbb{R}$. Así, nuestro conjunto de entrenamiento presenta la siguiente forma:

\[
    \mathcal{D} = \{ (x_i, y_i) \in \mathbb{R}^{D} \times \mathbb{R} \}, \; i \in \{ 1, \ldots, N \}.
\]\newline

Definimos $X \in \mathcal{M}_{N \times D}(\mathbb{R})$ como la matriz cuyas filas son los vectores $x_{i}^{T}$ e $y \in \mathbb{R}^{N}$ como el vector columna cuyos elementos son los $y_i$. Recordemos que la regla de actualización del descenso de gradiente para el vector de parámetros $w$, utilizando la función de pérdida $\mathcal{L}$ y una tasa de aprendizaje $\eta$, viene dada por:

\begin{equation}
    \mathbf{w}^{(\tau + 1)} = \mathbf{w}^{(\tau)} - \eta \nabla \mathcal{L}(\mathbf{w}).
    \label{eq:descenso-gradiente2}
\end{equation}\newline

De manera similar a la sección anterior, consideramos el siguiente problema de optimización para resolver la regresión lineal:

\begin{equation}\label{eq:regresion-lineal-gd}
    \min_{w \in \mathbb{R}^{D}} \mathcal{L}(w) = \min_{w \in \mathbb{R}^{D}} \frac{1}{2}\| Xw - y \|^{2}
\end{equation}

donde en la Ecuación~\eqref{eq:regresion-lineal-gd} se agrega el término $\frac{1}{2}$ para simplificar los cáculos de las derivadas en pasos posteriores. De esta forma, podemos enfocarnos en analizar las propiedades de la solución que encuentra el descenso de gradiente.\newline

\begin{teorema}
    El conjunto de soluciones de un problema de mínimos cuadrados, denotado por $\mathcal{S}_{LS}$, es exactamente el siguiente:

    \[
        \mathcal{S}_{LS} = \{ X^{\dagger}y + (I_D -X^{\dagger}X)u, \; u \in \mathbb{R}^{D} \}.
    \]
\end{teorema}

\begin{proof}
    Podemos expresar el error de la Ecuación~\eqref{eq:regresion-lineal-gd} de la siguiente forma: $Xw - y = Xw - XX^{\dagger}y +XX^{\dagger}y - y = Xw - XX^{\dagger}y - (I_D - XX^{\dagger})y$.\newline

    A partir de la última ecuación, sabemos que $XX^{\dagger}y$ representa la proyección de $y$ sobre el espacio columna de $X$ y $(I_D - XX^{\dagger})y$ corresponde a su componente ortogonal. Esto se debe a que $XX^{\dagger}y$ es la proyección sobre $Im(X)$ y $(I_D - XX^{\dagger})y$ es la proyección en el complemento ortogonal de $Im(X)$, es decir, el $\ker(X^{T})$. Para verificar estas afirmaciones, basta utilizar ambas propiedades de manera conjunta en el Lema~\ref{lema:propiedades-pseudoinversa}.\newline

    Dado que ambos términos son ortogonales, podemos descomponer la norma del error de la siguiente forma:

    \[
        \| Xw - y \|^{2} = \| Xw - XX^{\dagger}y \|^{2} + \| (I_D - XX^{\dagger})y \|^{2}
    \]

    lo que nos lleva a la siguiente desigualdad:

    \[
        \| Xw - y \|^{2} \geq \| (I_D - XX^{\dagger})y \|^{2}.
    \]

    Finalmente, la igualdad se alcanza si y solo si:

    \[
        Xw - XX^{\dagger}y = 0 \implies  Xw = XX^{\dagger}y.
    \]

    De esta manera, sabemos que $X^{\dagger}y$ es una solución particular de la ecuación $Xw = XX^{\dagger}y$. Para obtener la solución general, basta con añadir cualquier vector que se encuentre en el núcleo de $X$, es decir, cualquier vector de la forma $\{ (I_D-X^{\dagger}X)u, \; u \in \mathbb{R}^{D} \}$ (véase Lema~\ref{lema:propiedades-pseudoinversa}).\newline
\end{proof}

Una vez que conocemos el conjunto de soluciones para nuestro problema de optimización, podemos analizar cómo se comporta dicho conjunto en función del rango de la matriz $X$.

\begin{observacion}
    Dependiendo del rango de la matriz $X$, el conjunto de soluciones $\mathcal{S}_{LS}$ variará en función de la expresión de la pseudoinversa $X^{\dagger}$:

    \begin{itemize}
        \item Si $N < D$ y $rang(X) = N$, entonces $X^{\dagger} = X^{T}{(XX^{T})}^{-1}$ y $\mathcal{S}_{LS} = \{ X^{T}{(XX^{T})}^{-1}y + (I_D-X^{T}{(XX^{T})}^{-1}X)u, \; u \in \mathbb{R}^{D} \}$.
        
        \item Si $D < N$ y $rang(X) = D$, entonces $X^{\dagger} = {(X^{T}X)}^{-1}X^{T}$ y $\mathcal{S}_{LS} = \{ {(X^{T}X)}^{-1}X^{T}y \}$.
        
        \item Si $D = N$ y $X$ tiene inversa, entonces $X^{\dagger} = {X}^{-1}$ y $\mathcal{S}_{LS} = \{{X}^{-1}y \}$.
    \end{itemize}

    En particular, en los dos últimos casos, dado que $\ker(X)$ es trivial, la solución del problema es única.\newline
\end{observacion}

Por tanto, nos interesa centrarnos en analizar qué tan buena es la solución que proporciona el descenso de gradiente en la región sobreparametrizada, es decir, cuando $N < D$, dado que es en este caso cuando se generan múltiples soluciones, mientras que en los otros casos la solución es única.

\begin{teorema}
    Si el problema de mínimos cuadrados lineales~\eqref{eq:regresion-lineal-gd} se encuentra en la región sobreparametrizada, es decir, $(N < D)$ y $rang(X) = N$, entonces, usando el descenso de gradiente con una tasa de aprendizaje constante $0 < \eta < \frac{1}{\lambda_{\max}(X)}$, donde $\lambda_{\max}(X)$ es el mayor valor propio de $X$ y partiendo desde un punto inicial $w_0 \in Im(X^{T})$, se garantiza la convergencia hacia la solución de norma mínima.
\end{teorema}

\begin{proof}
    Dado que estamos suponiendo que $X$ tiene $N$ filas linealmente independientes, podemos expresar su descomposición en valores singulares de la siguiente manera:

    \[
        X = U \Sigma V^T = \begin{bmatrix} U_1 & U_2 \end{bmatrix} \begin{bmatrix} \Sigma_1 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} V_1^T \\ V_2^T \end{bmatrix}
    \]

    donde $U \in \mathbb{R}^{N \times N}$ y $V \in \mathbb{R}^{D \times D}$ son matrices ortogonales, $\Sigma \in \mathbb{R}^{N \times D}$ es una matriz diagonal rectangular, $U_1$ (respectivamente $V_1$) son las submatrices que contienen los vectores singulares izquierdos (respectivamente derechos) asociados con los valores singulares no nulos y $\Sigma_1 \in \mathbb{R}^{n \times n}$ es una matriz diagonal con valores singulares no nulos.\newline

    Nos centramos ahora en encontrar la solución de norma mínima $w^{*}$, la cual, como sabemos, viene dada por la pseudoinversa:

    \[
        w^* = X^T {(X X^T)}^{-1} y.
    \]\newline

    Utilizando la descomposición en valores singulares de las matrices $X$ y $X^{T}$, y dado que $V$ es ortogonal, obtenemos:

    \[
        X X^T = U \Sigma V^{T} V \Sigma^{T} U^{T} = U \Sigma \Sigma^{T} U^{T}.
    \]\newline

    Así, para la inversa de la matriz ${(X X^T)}$, sabiendo que $U$ es ortogonal, nos queda:

    \[
        {(X X^T)}^{-1} = U {(\Sigma \Sigma^{T})}^{-1} U^{T}.
    \]\newline

    Volviendo a usar la descomposición en valores singulares para la matriz $X^{T}$, $X^{T} = V \Sigma U^{T}$, obtenemos que la solución de norma mínima $w^{*}$ se puede reescribir como sigue (los vectores singulares asociados a los valores singulares nulos no tienen impacto en la solución de mínimos cuadrados, pues su dirección es linealmente dependiente):

    \[
        w^* = X^T {(X X^T)}^{-1} y = V \Sigma U^{T} U {(\Sigma \Sigma^{T})}^{-1} U^{T}y = V_1 \Sigma_1^{-1} U_{1}^T y.
    \]\newline

    A continuación, trabajamos sobre la regla de actualización del gradiente descendente, que viene dada por:
    \[
        w^{(k+1)} = w^{(k)} - \eta \nabla \mathcal{L}(w) = w^{(k)} - \eta X^T (Xw^{(k)} - y) = (I - \eta X^T X)w^{(k)} + \eta X^T y
    \]

    donde $X^T (Xw^{(k)} - y)$ hace referencia a la derivada de $\mathcal{L}(w)$ respecto de $w$.\newline

    Seguidamente, aplicando inducción, obtenemos la expresión general:
    \[
        w^{(k)} = (I - \eta X^T X)^k w_0 + \eta \sum_{l=0}^{k-1} (I - \eta X^T X)^l X^T y.
    \]\newline


    Ahora bien, usando la descomposición en valores singulares para la matriz $X^T X$ $(V \Sigma^T \Sigma V^T)$ y dado que $V$ es ortogonal, la iteración del gradiente descendente para el paso $k$-ésimo se expresa como:
    \[
        w^{(k)} = V (I - \eta \Sigma^T \Sigma)^k V^T w_0 + \eta V \left(\sum_{l=0}^{k-1} (I - \eta \Sigma^T \Sigma)^l \Sigma^T \right)U^T y.
    \]\newline

    Reescribiendo la ecuación anterior en términos de $\Sigma_1$, obtenemos:
    \[
        w^{(k)} = V \begin{bmatrix} (I - \eta \Sigma_1^2)^k & 0 \\ 0 & I \end{bmatrix} V^T w_0 + \eta V \left(\sum_{l=0}^{k-1} \begin{bmatrix} (I - \eta \Sigma_1^2)^l \Sigma_1 \\ 0 \end{bmatrix} \right) U^T y.
    \]

    De cara a garantizar la convergencia del descenso de gradiente, elegimos $0 < \eta < \frac{1}{\lambda_{\max}(\Sigma_1)}$, lo que asegura que los valores propios de $I - \eta \Sigma^T \Sigma$ sean estrictamente menores que $1$, lo que implica, a su vez, que:

    \[
        \left( I - \eta \Sigma_1^T \Sigma_1 \right)^k \to 0 \quad \text{cuando} \quad k \to \infty
    \]

    y, por tanto,

    \[
        V \begin{bmatrix} (I - \eta \Sigma_1^2)^k & 0 \\ 0 & I \end{bmatrix} V^T w_0 \xrightarrow{k \to \infty} V \begin{bmatrix} 0 & 0 \\ 0 & I \end{bmatrix} V^T w_0 = V_2 V_2^T w_0
    \]\newline

    dado que $V_{2}^{T}w_0$ es la parte de $w_0$ correspondiente a los valores singulares nulos de $X^{T}X$.\newline

    Además,
    \[
        \eta \sum_{l=0}^{k-1} \begin{bmatrix} (I - \eta \Sigma_1^2)^l \Sigma_1 \\ 0 \end{bmatrix}  \xrightarrow{k \to \infty} \eta \begin{bmatrix} \sum_{l=0}^{\infty}(I - \eta \Sigma_1^2)^l \Sigma_1 \\ 0 \end{bmatrix} = \begin{bmatrix} \eta(I - I + \eta \Sigma_1^2)^{-1} \Sigma_1 \\ 0 \end{bmatrix} = \begin{bmatrix} \Sigma_1^{-1} \\ 0 \end{bmatrix}
    \]\newline

    Por consiguiente, denotando $w_\infty$ como el límite de las iteraciones del gradiente descendente, obtenemos:
    \[
        w_\infty = V_2 V_2^T w_0 + V_1 \Sigma_1^{-1} U^T y = V_2 V_2^T w_0 + X^{T}{(XX^{T})}^{-1}y = V_2 V_2^T w_0 + w^*.
    \]\newline

    Finalmente, dado que $w_0$ está en la imagen de $X^T$, podemos escribir $w_0 = X^T z$ para algún $z \in \mathbb{R}^N $, lo que implica (usando la descomposición en valores singulares de la matriz $X^{T}$):
    \[
        V_2 V_2^T w_0 = V \begin{bmatrix} 0 & 0 \\ 0 & I \end{bmatrix}V^{T}X^{T}z = V \begin{bmatrix} 0 & 0 \\ 0 & I \end{bmatrix}V^{T}V\Sigma^{T}U^{T}z = V \begin{bmatrix} 0 & 0 \\ 0 & I \end{bmatrix}\begin{bmatrix} \Sigma_1 \\ 0 \end{bmatrix}U^{T} = 0.
    \]\newline

    En consecuencia, el término $V_2 V_2^T w_0$ desaparece, y nos queda que $w_\infty = w^{*}$, por lo que el gradiente descendente converge a la solución de norma mínima.\newline
\end{proof}

En conclusión, tanto el gradiente descendente como la pseudoinversa llegan a la misma solución (bajo ciertas hipótesis) cuando se trata de resolver un problema de regresión lineal, ya que ambos buscan la solución interpolante de norma mínima, lo que los convierte en métodos equivalentes. De esta manera, realizar el gradiente descendente hasta la convergencia, comenzando con una inicialización de todos los pesos a cero, es equivalente a aplicar directamente la pseudoinversa. Por tanto, podemos interpretar que utilizar la pseudoinversa para resolver un problema de mínimos cuadrados es, en esencia, un caso particular del gradiente descendente aplicado al mismo problema.\newline

\subsection{Problema de clasificación con datos separables}\label{subsec:problema-clasificación-separable}
En esta subsección, nos preocupamos de verificar el efecto de utilizar el descenso de gradiente como método de optimización en un problema de clasificación binaria con un conjunto de datos linealmente separable~\cite{Soudry2024}. Cabe destacar que, por simplicidad, condideramos un problema de clasificación binaria con un dataset separable, lo que facilita la intuición sobre el comportamiento del descenso de gradiente. No obstante, los principios que se discuten posteriormente pueden extenderse a problemas de clasificación multiclase~\cite{Ravi2024} y al uso del descenso de gradiente en redes neuronales~\cite{Gunasekar2019}.\newline

\begin{definicion}
    Un conjunto de datos $\mathcal{D} = \{ (x_i, y_i) \; | \; i \in \{1, \ldots, N \}\}$ donde para todo $i \in \{1, \ldots, N \}$ se verifica que $(x_i, y_i) \in \mathbb{R}^{D} \times \{-1, 1\}$, es linealmente separable si existe $w_{*} \in \mathbb{R}^{D}$ de manera que $\forall i : y_i w_{*} x_i > 0$.\newline
\end{definicion}

Además, los resutados que se exponen en este apartado se cumplen para funciones de pérdida $\ell: \mathbb{R} \to \mathbb{R}_{+}$ que presentan las siguientes propiedades:
\begin{enumerate}
    \item\label{prop:positiva} $\ell$ es positiva, diferenciable y decreciente de manera monótona a cero $(\ell(u)>0, \ell'(u)<0 \; \text{y} \; \lim \limits_{u \to \infty} \ell(u) = \lim \limits_{u \to \infty} \ell'(u) = 0)$.
    \item\label{prop:smooth} $\ell$ es una función $\beta$-suave, es decir, el gradiente de $\ell$ es $\beta$-Lipschitz $(\forall u,v \in \mathbb{R}, \; \| \nabla \ell(u) - \nabla \ell(v) \| \leq \beta \| u-v \|)$.
    \item\label{prop:derivada_negativa} $\lim \limits_{u \to -\infty} \ell'(u) < 0$.\newline
\end{enumerate}

Estas propiedades incluyen funciones de pérdida comunes para problemas de clasificación binaria, incluyendo la función logística y la exponencial. Además, estas propiedades implican que $\mathcal{L}(w)$ es una función $\beta \sigma^{2}_{\max}(X)$-suave (ligado al hecho de que $\ell$ es $\beta$-suave), donde $\sigma_{\max}(X)$ es el máximo valor singular de la matriz $X \in \mathbb{R}^{D \times N}$ que contiene los datos de entrenamiento.\newline

Por tanto, para nuestro problema, consideramos un conjunto de entrenamiento $\mathcal{D}$ linealmente separable formado por $N$-pares de datos de la forma $(x_i, y_i)$ con $x_i \in \mathbb{R}^{D}$ y $y_i \in \{ -1, 1\}$, donde $i \in \{1, \ldots, N \}$ y nos centramos en minimizar la función de pérdida dada por:

\[
    \min_{w \in R^{D}}\mathcal{L}(w) = \min_{w \in R^{D}} \sum_{i=1}^{N} \ell(y_i w^{T}x_i)
\]

donde $w \in \mathbb{R}^{D}$ es el vector de parámetros y $\ell$ es una función de pérdida binaria verificando las propiedades~\ref{prop:positiva},~\ref{prop:smooth} y~\ref{prop:derivada_negativa} descritas anteriormente.\newline

Seguidamente, nos ocupamos de estudiar la solución que ofrece el descenso de gradiente para este problema, fijada una tasa de aprendizaje $\eta$:

\[
    \mathbf{w}^{(\tau + 1)} = \mathbf{w}^{(\tau)} - \eta \nabla \mathcal{L}(\mathbf{w}) = \mathbf{w}^{(\tau)} - \eta \sum_{i=1}^{N} \ell'(y_i w^{T}x_i)y_i x_i.
\]\newline

De cara a estudiar esta solución, utilizaremos un lema auxiliar que nos ayudará con la demostración del resultado principal de esta subsección.
\begin{lema}\label{lema:raro-clasificación-gd}
    Sea $\mathcal{L}(w)$ una función $\beta$-suave no negativa. Si $\eta < \frac{2}{\beta}$, entonces, para cualquier $w_0$ y utilizando el método del descenso de gradiente dado por:

    \[
        \mathbf{w}^{(\tau + 1)} = \mathbf{w}^{(\tau)} - \eta \nabla \mathcal{L}(\mathbf{w})
    \]

    se tiene que $\sum_{u=0}^{\infty} \| \nabla\mathcal{L}(w^{(u)}) \|^{2} < \infty$ y, por tanto:

    \[
        \lim \limits_{t \to \infty} \| \nabla\mathcal{L}(w^{(t)}) \|^{2} = 0.
    \]
\end{lema}

La demostración del lema se puede verificar en el Apéndice~\ref{ap:apendiceA}.\newline

Una vez conocemos el resultado anterior, podemos enunciar el principal teorema de esta subsección, que establece las condiciones necesarias para que el descenso de gradiente converja hacia un punto crítico.

\begin{teorema}\label{teorema:clasificación-gd}
    Sea $\mathcal{D}$ un conjunto de entrenamiento linealmente separable con $N$ ejemplos y $\ell: \mathbb{R} \to \mathbb{R}_{+}$ una función de pérdida verificando las propiedades~\ref{prop:positiva},~\ref{prop:smooth} y~\ref{prop:derivada_negativa}. Sean $w_t$ las iteraciones del descenso de gradiente usando una tasa de aprendizaje $0 < \eta < \frac{2}{\beta \sigma^{2}_{\max}(X)}$ y cualquier punto inicial $w_0$. Entonces, se cumplen:

    \begin{enumerate}
        \item $\lim \limits_{t \to \infty} \mathcal{L}(w^{(t)}) = 0$.
        \item $\lim \limits_{t \to \infty} \| w^{(t)} \| = \infty$.
        \item $\forall i \in \{ 1, \ldotp, N\}: \; \lim \limits_{t \to \infty} y_i {w}^{(t)T} x_i = \infty$.
    \end{enumerate}
\end{teorema}

\begin{proof}
    Dado que el conjunto de entrenamiento $\mathcal{D}$ es linealmente separable, $\exists w_{*} \in \mathbb{R}^{D}$ de manera que $\forall i \in \{1, \ldots, N \} : y_i w_{*} x_i > 0$ y, entonces

    \[
        w_{*}^{T}\nabla\mathcal{L}(w) = \sum_{i=1}^{N} \underbrace{\ell'(y_i w^{T} x_i)}_{< 0} \underbrace{y_i w_{*}^{T}x_i}_{> 0} < 0.
    \]

    Para cualquier $w$ finito, la suma anterior no puede ser igual a $0$, ya que es una suma de términos negativos $(\forall i: y_i w_{*}^{T}x_i > 0)$ y $\forall u: \ell'(u) < 0$, debido a las propiedades que cumple la función de pérdida $\ell$. Por lo tanto, no hay puntos críticos finitos $w$ para los cuales $\nabla \mathcal{L}(w) = 0$. Sin embargo, el descenso de gradiente en una función de pérdida suave con una tasa de aprendizaje adecuada siempre garantiza converger a un punto crítico, es decir, $\nabla \mathcal{L}(w^{(t)}) \to 0$ (véase Lema~\ref{lema:raro-clasificación-gd}). Este hecho implica necesariamente que $\| w^{(t)} \| \to \infty$, que es la condición \textbf{(2)}, mientras se verifique que $\exists t_0 : \; \forall t > t_0, \; \forall i: y_i w_{t}^{T} x_i > 0$, que es la condición \textbf{(3)}, pues únicamente entonces se cumple $\ell'(y_i {w}^{(t)T} x_i) \to 0$. Entonces, se verifica que $\mathcal{L}(w^{(t)}) \to 0$ (condición \textbf{(1)}) y, por tanto, el descenso de gradiente converge hacia un mínimo global.\newline
\end{proof}

El Teorema~\ref{teorema:clasificación-gd} nos asegura que el descenso de gradiente, eligiendo una tasa de aprendizaje adecuada, converge hacia un mínimo global, dado que $\mathcal{L}(w^{(t)}) \to 0$. Además, no se requiere que la función de pérdida $\mathcal{L}(w)$ sea convexa, lo que amplia la aplicabilidad del resultado a una clase más general de funciones.\newline

\section{Optimización en la zona sobreparametrizada}\label{sec:optimizacion-zona-sobreparametrizada}

Como hemos observado en secciones anteriores, la sobreparametrización resulta beneficiosa dentro del marco del aprendizaje estadístico. Este comportamiento, aunque contraintuitivo como se indica en~\cite{Hastie2001}, que afirma: ``Un modelo con error de entrenamiento cero sobreajusta los datos de entrenamiento y, por lo general, generalizará mal'', refleja cómo las soluciones de los sistemas sobreparametrizados pueden mejorar la generalización.\newline

Como hemos observado en secciones anteriores, la sobreparametrización resulta beneficiosa dentro del marco del aprendizaje estadístico. Este comportamiento, aunque contraintuitivo~\cite{Hastie2001} ``Un modelo con cero error de entrenamiento está por encima de los datos de entrenamiento y, por lo general, generalizará mal'', refleja cómo el las soluciones de los sistemas sobreparametrizados pueden mejorar la generalización.\newline

Sin embargo, desde el punto de vista de la optimización, que se concentra en los algoritmos y técnicas utilizadas para encontrar el mejor modelo posible, la sobreparametrización también ofrece ventajas, ya que facilita la convergencia de los métodos de optimización hacia un mínimo global, especialmente en aquellos que emplean el descenso de gradiente o alguna extensión del mismo.\newline

En el caso de que nuestra función de pérdida $\ell: \mathcal{Y} \to \mathbb{R}$ sea convexa y nuestro modelo $g: \mathcal{X} \to \mathcal{Y}$, elegido del conjunto de hipótesis, sea lineal, es fácil observar que $l \circ f$ es convexa (basta con aplicar las propiedades de la linealidad de $g$ junto con las propiedades de convexidad de $\ell$). Como resultado, se garantiza la convergencia hacia la mejor solución, es decir, el mínimo global de la función de pérdida, sin riesgo de quedar atrapados en mínimos locales.\newline

No obstante, en el marco del aprendizaje profundo, el modelo resultante es no lineal debido al uso de funciones de activación no lineales, lo que provoca que, en general, el paisaje de la función de pérdida sea no convexo. Esto implica que los métodos de optimización de primer orden, como el GD, puedan quedar atrapados en mínimos locales, ya que solo utilizan información local del gradiente, dependiendo así de su inicialización.\newline

Como resultado, los sistemas sobreparametrizados dan lugar a paisajes de la función de pérdida que son \textit{esencialmente no convexos}, es decir, por lo general no existe un vecindario alrededor de ningún minimizador global en el que el paisaje de la función de pérdida sea convexo. Esto contrasta con lo que ocurre en los sistemas infraparametrizados, donde dicho vecindario suele existir, aunque sea extremadamente pequeño (véase Figura~\ref{fig:localglobalminima}).\newline

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{img/localglobalminima.png}
    \caption[Ejemplos de paisajes de la función de pérdida en redes neuronales~\cite{Liu2021}.]{Ejemplos de paisajes de la función de pérdida en redes neuronales~\cite{Liu2021}. A la izquierda, se muestra un paisaje localmente convexo alrededor de los mínimos locales, ligado a la zona infraparametrizada. A la derecha, se muestra un paisaje incompatible con la convexidad local con múltiples mínimos globales, ligado a la zona sobreparametrizada.}\label{fig:localglobalminima}
\end{figure}

Con el fin de estudiar de manera más formal los resultados presentados, consideramos nuevamente un problema típico del aprendizaje supervisado, con un dataset $\mathcal{D}$ compuesto por $N$-pares de ejemplos de entrenamiento, es decir,  $\mathcal{D} = \{(x_i, y_i)\}_{1}^{N}$, con $x_i \in \mathbb{R}^{D}$ e $y \in \mathbb{R}$. Si consideramos nuestro conjunto de hipótesis $\mathcal{H}$ como una familia paramétrica de modelos, como, por ejemplo, una red neuronal, nuestro objetivo es encontrar el vector de parámetros óptimo $w^{*}$, de manera que el modelo se ajuste a los datos de entrenamiento, es decir

\[
    f(w^{*}; x_i) \approx y_i, \quad i \in \{1,\ldots,n \}.
\]

Matemáticamente, esto es equivalente a resolver un sistema con $N$ ecuaciones\footnote{Para problemas de clasificación multiclase o problemas de regresión con múltiples salidas, donde $C$ es el número de clases o salidas distintas, estaremos resolviendo un sistema de $N \times C$ ecuaciones.}. Agregándolas todas en una única función, podemos escribir:

\begin{equation}\label{eq:optimizacion-uno}
    \mathcal{F}(w) = y, \; \text{donde} \; w \in \mathbb{R}^{P}, y \in \mathbb{R}^{N}, \mathcal{F}(\cdot):\mathbb{R}^{P} \to \mathbb{R}^{N}.
\end{equation}

De esta manera, $(\mathcal{F}(w)) = f(w; x_i)$. Una solución exacta de la Ecuación~\eqref{eq:optimizacion-uno} corresponde a la interpolación.\newline

Usamos $\mathcal{DF}$ para representar la derivada de la función $\mathcal{F}$, donde $\mathcal{DF} \in \mathcal{M}_{N \times P}(\mathbb{R})$, con $(\mathcal{DF})_{ij} = \frac{\partial \mathcal{F}_{i}}{\partial w_j}$. Denotamos la matriz hessiana de la función $\mathcal{F}$ como $H_{\mathcal{F}}$, que es un tensor de tamaño $N \times P \times P$ con $(H_{\mathcal{F}})_{ijk} = \frac{\partial^2 \mathcal{F}_{i}}{\partial w_j \partial w_k}$, y definimos la norma del tensor hessiano como $\| H_{\mathcal{F}} \| = \max_{i \in \{1, \ldots,N \}} \| H_{\mathcal{F}_{i} \|}$, donde $H_{\mathcal{F}_{i}} = \frac{\partial^2 \mathcal{F}_{i}}{\partial w^2}$. De manera análoga, denotamos la matriz hessiana de la función de pérdida como $H_{\mathcal{L}} = \frac{\partial^2 \mathcal{L}}{\partial w^2}$.\newline

Para detallar de manera analítica la no convexidad en la zona sobreparametrizada, se presenta el siguiente resultado.

\begin{proposicion}\label{prop:non-conexity}
    Sea $w^{*}$ una solución, es decir, $\mathcal{L}(w^{*}) = 0$, y supongamos que $\frac{d}{dw}\frac{\partial \mathcal{L}}{\partial \mathcal{F}}(w^{*}) \neq 0$ y $rang(H_{\mathcal{F}_{i}}(w^{*})) > 2N$ para algún $i \in \{1, \ldots, N \}$. Entonces $\mathcal{L}(w)$ no es convexa en ninguna vecindad de $w^{*}$.
\end{proposicion}

\begin{proof}
    Se presenta una idea intuitiva de cómo realizar la demostración del resultado anterior. Para ello, tomaremos dos puntos distintos del vecindario de la solución $w^*$ y evaluaremos las matrices hessianas de la función de pérdida en dichos puntos. Una vez evaluadas, podemos verificar que alguna de las matrices anteriores es no negativa, lo que implica que la función de pérdida $\mathcal{L}(w)$ no es localmente convexa alrededor de $w^*$, ya que la hessiana no es semidefinida positiva en esa vecindad, lo cual es una condición necesaria para que la función sea localmente convexa.\newline

    Por último, se referencia al lector más curioso al Apéndice~\ref{ap:apendiceB} para la demostración detallada de la proposición.\newline
\end{proof}

Para el caso de sistemas infraparametrizados, los mínimos locales se encuentran generalmente aislados. Dado que $H_{\mathcal{L}}(w^{*})$ es definida positiva cuando $w^{*}$ es un mínimo aislado, por la continuidad de $H_{\mathcal{L}}(\cdot)$, la definición de positivad se preserva en la vecindad de $w^{*}$. Por consiguiente, $\mathcal{L}(w)$ es localmente convexa alrededor de $w^{*}$.\newline

A su vez, en los sistemas sobreparametrizados contamos con más parámetros que constantes. En este caso, el sistema de ecuaciones general tiene múltiples soluciones exactas, las cuales forman una variedad continua de dimensión $P - N > 0$, de modo que ninguna de estas soluciones está aislada. Este hecho puede observarse en la Figura~\ref{fig:localglobalminima} y se encuentra demostrado en el Apéndice A de~\cite{Liu2021}. En cuanto a nuestro interés, podemos concluir que, para redes suficientemente parametrizadas, los mínimos globales siempre están rodeados por otro mínimo global en un vecindario relativamente pequeño.\newline

Estos resultados nos conducen a la conclusión de que, para el análisis de los sistemas sobreparametrizados, no podemos utilizar la convexidad, ni siquiera localmente, como base. En consecuencia, es necesario buscar una condición, a priori, más simple, que verifiquen las funciones de pérdida con el fin de analizar el comportamiento de dichos sistemas. Para ello, se recurre a una modificación de la condición de Polyak y Lojasiewicz.\newline

\begin{definicion}[Condición $\mu$-$PL$]
    Decimos que una función no negativa $\mathcal{L}$ verifica la condición $\mu$-$PL$ (Polyak-Lojasiewicz) en un conjunto $\mathcal{S} \subset \mathbb{R}^{P}$ para $\mu > 0$, si

    \[
        \| \nabla \mathcal{L}(w) \|^{2} \geq \mu \mathcal{L}(w), \; \forall w \in \mathcal{S}.
    \]\newline
\end{definicion}

\begin{teorema}
    Supongamos que la función de pérdida $\mathcal{L}(w)$ es $\beta$-suave y cumple la condición $\mu$-$PL$ en la bola $B(w_0, R) = \{ w \in \mathbb{R}^{P} : \| w - w_0 \| \leq R \}$ con $R = \frac{2 \sqrt{2 \beta \mathcal{L}(w_0)}}{\mu}$. Entonces se tiene:

    \begin{enumerate}
        \item (\textbf{Existencia de una solución}). Existe una solución (minimizador global de $\mathcal{L}$) $w^{*} \in B(w_0, R)$, de manera que $\mathcal{F}(w^{*}) = y$.
        \item (\textbf{Convergencia del GD}). El gradiente descendente con un learning rate $\eta \leq \frac{1}{\sup_{w \in B(w_0, R)} \| \mathcal{H}_{\mathcal{L}}(w) \|}$ converge a una solución global en $B(w_0, R)$.
    \end{enumerate}
\end{teorema}

\begin{proof}
    Probaremos este teorema por inducción, sabiendo que nuestra hipótesis de inducción nos dice que, para todo $t \geq 0$, $w_t$ está dentro de la bola $B(w_0, R)$ con $R = \frac{2\sqrt{2\beta \mathcal{L}(w_0)}}{\mu}$.\newline

    En el caso inicial, cuando $t = 0$, es trivial que $w_0 \in B(w_0, R)$. Supongamos que, por inducción, para un $t \geq 0$, $w_t$ está en la bola $B(w_0, R)$. Para probar que $w_{t+1} \in B(w_0, R)$, usando que la función de pérdida $\mathcal{L}$ es $\beta$-suave, tenemos que:
    \begin{align}
        \| w_{t+1} - w_0 \| &= \eta \bigg\| \sum_{i=0}^{t} \nabla \mathcal{L}(w_i) \bigg\| \\
        &\leq \eta \sum_{i=0}^{t} \| \nabla \mathcal{L}(w_i) \| \\
        &\leq \eta \sum_{i=0}^{t} \sqrt{2 \beta (\mathcal{L}(w_i) - \mathcal{L}(w_{i+1}))} \\
        &\leq \eta \sum_{i=0}^{t} \sqrt{2 \beta \mathcal{L}(w_i)} \\
        &\leq \eta \sqrt{2\beta} \left( \sum_{i=0}^{t} (1 - \eta \mu)^{i/2} \right) \sqrt{\mathcal{L}(w_0)} \\
        &\leq \eta \sqrt{2\beta} \sqrt{\mathcal{L}(w_0)} \frac{1}{1 - \sqrt{1 - \eta \mu}} \\
        &\leq \frac{2 \sqrt{2\beta} \sqrt{\mathcal{L}(w_0)}}{\mu} = R.
    \end{align}

    Por tanto, $w_{t+1}$ se encuentra en la bola $B(w_0, R)$.\newline
\end{proof}

Finalmente, se presenta una idea intuitiva, utilizando la función de pérdida cuadrática, de por que los paisajes de pérdida de los sistemas sobreparametrizados satisfacen la condición $\mu$-PL en la mayor parte de su espacio de parámetros.\newline

\begin{definicion}
    El núcleo tangente de la función $\mathcal{F}$ en el vector $w$ se define como

    \[
        K(w) = \mathcal{DF}(w)\mathcal{DF}^{T}(w)
    \]

    donde $K(w) \in \mathcal{M}_{N \times P}(\mathbb{R})$ es una matriz semidefinida positiva.\newline
\end{definicion}

\begin{definicion}[Condicionamiento uniforme]
    Decimos que $\mathcal{F}(w)$ está $\mu$-uniformemente condicionada $(\mu > 0)$ en un subconjunto $\mathcal{S} \subset \mathbb{R}^{P}$ si el valor propio más pequeño de su núcleo tangente satisface

    \[
        \lambda_{\min}(K(w)) \geq \mu, \; \forall w \in \mathcal{S}.
    \]\newline
\end{definicion}

El siguiente resultado muestra cómo el hecho de estar condicionada de manera uniforme es una condición suficiente para que la pérdida cuadrática cumpla la condición $\mu$-PL.

\begin{teorema}[Condicionamiento uniforme $\implies$ Condición $\mu$-PL]
    Si $\mathcal{F}(w)$ está $\mu$-uniformemente condicionada en un conjunto $\mathcal{S} \subset \mathbb{R}^P$, entonces la función de pérdida cuadrática $\mathcal{L}(w) = \frac{1}{2} \|\mathcal{F}(w) - y \|^2$ satisface la condición $\mu$-PL$^*$ en $\mathcal{S}$.
\end{teorema}

\begin{proof}
    \[
        \begin{aligned}
            \frac{1}{2} \|\nabla \mathcal{L}(w)\|^2 &= \frac{1}{2} (\mathcal{F}(w) - y)^T K(w) (\mathcal{F}(w) - y) \\
            &\geq \frac{1}{2} \lambda_{\min}(K(w)) \|\mathcal{F}(w) - y \|^2 = \lambda_{\min}(K(w)) \mathcal{L}(w) \geq \mu \mathcal{L}(w).
        \end{aligned}
    \]

\end{proof}

Nos centramos ahora en mostrar una intuición de por qué la pérdida cuadrática cumple la condición $\mu$-PL en la mayor parte del espacio de parámetros. La observación clave viene dada por el rango del núcleo tangente:

\[
        rang(K(w)) = rang((D\mathcal{F}(w) D\mathcal{F}^T(w))) = rang(D\mathcal{F}(w))
\]

donde $K(w)$ es, por definición, una matriz semidefinida positiva. Por tanto, el conjunto singular $\mathcal{S}_{\text{sing}}$, donde el núcleo tangente es degenerado, se puede escribir como

\[
    \mathcal{S}_{\text{sing}} = \{w \in \mathbb{R}^P \mid \lambda_{\min}(K(w)) = 0\} = \{w \in \mathbb{R}^P \mid \text{rango } D\mathcal{F}(w) < n\}.
\]

Así, tenemos que $rang(K(w)) = \min(P, N)$. Para el caso más simple, consideremos $N = 1$. En este caso, el núcleo tangente es un escalar y $K(w) = \| \mathcal{DF}(w) \|^2$ es singular si y solo si $D\mathcal{F}(w) = 0$. Como resultado, mediante un análisis de conteo de parámetros, esperamos que el conjunto singular $\mathcal{S}_{\text{sing}} = \{ w \mid K(w) = 0 \}$ tenga codimensión $P$ y, en consecuencia, consista únicamente de puntos aislados.\newline

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{img/cosarara2.png}
    \caption[Ejemplo de función de pérdida que satisface la condición $\mu$-PL~\cite{Liu2021}.]{Ejemplo de función de pérdida que satisface la condición $\mu$-PL~\cite{Liu2021}. La función de pérdida es $\mu$-PL en la zona sombreada. El conjunto singular corresponde a los vectores $w$ con núcleo tangente degenerado. Cada bola de radio $R = O\left(\frac{1}{\mu}\right)$ dentro del dominio $\mu$-PL se interseca con el conjunto de mínimos globales de la función de pérdida.}\label{fig:cosarara2}
\end{figure}

Aplicando un razonamiento similar, cuando $P > N$, esperamos que $\mathcal{S}_{\text{sing}}$ tenga codimensión positiva ($P - N + 1$) y sea un conjunto de medida cero. Esto significa que, dentro de un conjunto compacto, para valores suficientemente pequeños de $\mu$, es probable encontrar puntos que no satisfacen la condición $\mu$-PL alrededor de $\mathcal{S}_{\text{sing}}$, el cual es un subconjunto de baja dimensión en $\mathbb{R}^P$. Esto puede observarse en la Figura~\ref{fig:cosarara2}.\newline

Es importante notar que, cuanto mayor sea el grado de sobreparametrización del sistema, mayor será la codimensión esperada del conjunto singular. En otras palabras, la región donde la condición $\mu$-PL no se cumple se vuelve más pequeña en comparación con el espacio total de parámetros.\newline

En contraste, cuando $P < N$, el núcleo tangente es siempre degenerado ($\lambda_{\min}(K(w)) \equiv 0$), por lo que tales sistemas no pueden estar uniformemente condicionados y no pueden satisfacer la condición $\mu$-PL.\newline

\subsection{Elección de la mejor hipótesis}\label{subsec:suavidad-funcional}

Si bien los resultados presentados anteriormente aseguran la existencia y convergencia, en la zona sobreparametrizada, hacia un minimizador global de la función de pérdida, sabemos que en dicho régimen existe una multitud de minimizadores globales, es decir, cualquier hipótesis $g \in S \subset \mathcal{H}$.\newline  

A pesar de que todavía no existe una propuesta concluyente para seleccionar, de entre todas las hipótesis de $S$, aquella que mejor generaliza, se sabe que, aunque todas sean minimizadoras globales (es decir, satisfacen $\mathcal{L}(w) = 0$), no necesariamente todas garantizan una buena generalización. Un ejemplo claro de esto se puede observar para el caso de regresores polinómicos en la Figura~\ref{fig:suavidad1}, donde, una vez que tenemos suficientes parámetros para ajustar todos los datos de entrenamiento, empezamos a obtener distintos modelos que son minimizadores globales.\newline

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{img/suavidad1.png}
    \caption[Distintos modelos polinómicos para aproximar una función.]{Distintos modelos polinómicos (línea naranja) para aproximar una función objetivo (línea azul). Ambos modelos se ajustan de manera perfecta a los datos de entrenamiento (puntos azules); sin embargo, el segundo modelo generaliza mejor al estar regularizado hacia una solución de norma pequeña. Imagen del autor inspirada en~\cite{Schaeffer2023}.}\label{fig:suavidad1}
\end{figure}

Extrayendo conclusiones de la Figura~\ref{fig:suavidad1} y apoyándonos en la intuición ampliamente compartida dentro de la comunidad científica, se puede postular que una noción adecuada de \textbf{suavidad funcional} podría desempeñar un papel fundamental en la selección del mejor modelo con miras a la generalización.\newline  

La idea de maximizar la suavidad de una función sujeta a la interpolación de los datos representa una forma de actuar según el principio de la navaja de Ockham~\cite{Blumer1987}. Este principio establece que se debe preferir la explicación más simple que sea consistente con las evidencias. En nuestro caso, podemos utilizar como analogía que los datos de entrenamiento representan las evidencias, mientras que la simplicidad se asocia con la ``suavidad'' de la hipótesis a elegir.\newline

De esta manera, el principio de la máxima suavidad se puede formular como: ``Elegir la función más suave, de acuerdo con alguna noción de suavidad funcional, entre todas las que son minimizadores globales''~\cite{Belkin2021}.\newline

Este comportamiento también se encuentra asociado, en cierta medida, al conjunto de hipótesis $\mathcal{H}$ del modelo. Si consideramos dos conjuntos de hipótesis $\mathcal{H}_1$ y $\mathcal{H}_2$, de manera que $\mathcal{H}_1 \subset \mathcal{H}_2$, y sus correspondientes subconjuntos que contienen las hipótesis que son minimizadoras globales, $S_1 \subset \mathcal{H}_1$ y $S_2 \subset \mathcal{H}_2$, entonces, estos conjuntos estan relacionados por la misma inclusión ($S_1 \subset S_2$).\newline

Por tanto, si $\| \cdot \|_{s}$ es cualquier norma funcional, o más generalmente, cualquier funcional, se verifica que

\[
    \min_{g \in S_2} \| g \|_{s} \leq \min_{g \in S_1} \| g \|_{s}.
\]

Suponiendo que $\| \cdot \|_{s}$ es el sesgo inductivo correcto, que mide esa suavidad (por ejemplo, una norma de Sobolev), entonces esperamos que la hipótesis elegida de $S_2$ sea más suave, debido a que este conjunto contiene un mayor número de hipótesis que el conjunto $S_1$ y, por tanto, este conjunto es más adecuado para resolver el problema, lo que lleva a una mejor generalización de la hipótesis elegida.\newline

Como conlusión, podemos decir que el uso de sistemas sobreparametrizados resulta ventajoso para obtener cada vez más soluciones posibles (minimizadores globales) para nuestro problema. De esta forma, al aumentar la capacidad del modelo (en nuestro caso, su complejidad efectiva), resultará más sencillo para el algoritmo de optimización elegir una ``buena'' hipótesis (que generalice mejor que el resto) basada en algún sesgo inductivo que utilice. Este sesgo parece ir ligado hacia soluciones más simples, donde, en el caso de las hipótesis, se pueden entender como más ``suaves'', lo que conduce a la hipótesis elegida a una especie de ``autoregulación''.\newline

\section{Resto de desarrollos a realizar}\label{}

\section{Aproximación no lineal}\label{sec:aproximacion-no-lineal}

\subsection{Analogía con el deep double descent}

\section{Conclusión}\label{sec:conclusion-matematica}

\begin{table}[h]
    \centering
    \small 
    \renewcommand{\arraystretch}{0.9} 
    \begin{NiceTabular}{c|c|c|}[color-inside]
        \Block{3-1}{} & \Block[draw, fill={cyan!50}]{3-1}{\textbf{Régimen clásico} \\ \textbf{(infraparametrizado)}} & \Block[draw, fill={cyan!50}]{3-1}{\textbf{Régimen moderno} \\ \textbf{(sobreparametrizado)}} \\ \\ \\
        
        \hline
        \Block[draw]{3-1}{Curva de \\ generalización} & \Block[draw]{3-1}{Forma clásica de ``U''} & \Block[draw]{3-1}{Descendiente} \\ \\ \\

        \hline
        \Block[draw]{5-1}{Paisaje de \\ \\ optimización} & \Block[draw]{5-1}{Localmente convexo \\ \\ Mínimos locales aislados} & \Block[draw]{5-1}{No localmente convexo \\ Múltiples mínimos globales formando \\ variedades continuas \\ Cumple la condición $\mu$-PL} \\ \\ \\ \\ \\

        \hline
        \Block[draw]{3-1}{Modelo óptimo} & \Block[draw]{3-1}{\text{Sweet spot} (mínimo de la curva ``U'') \\ (difícil de conseguir)} & \Block[draw]{3-1}{Cualquier minimizador global \\ (fácil de encontrar)} \\ \\ \\

        \hline
        \Block[draw]{3-1}{Convergencia del GD} & \Block[draw]{3-1}{GD converge al mínimo local} & \Block[draw]{3-1}{GD converge a un mínimo global} \\ \\ \\
        \hline

    \end{NiceTabular}
    \caption{}\label{tabla:}
\end{table}

\endinput