% !TeX root = ../tfg.tex
% !TeX encoding = utf8

\chapter{Conclusiones}\label{ch:conclusiones}

La base teórica que sustenta los avances empíricos constituye una herramienta fundamental para el devenir del aprendizaje automático, y su desarrollo resulta esencial para comprender los nuevos comportamientos que van surgiendo en este ámbito. En el presente TFG, nos enfocamos en dar explicación al \textit{Deep Double Descent} a partir del desarrollo de nuevos conceptos dentro del aprendizaje, unificando la sabiduría clásica con la experimentación moderna.

En primer lugar, se realizó un análisis exhaustivo del estado del arte relacionado con las distintas manifestaciones del suceso, constatando su novedosa y creciente relevancia en la comunidad científica, así como identificando sus principales desencadenantes tanto en problemas de regresión como de clasificación.

A continuación, nos centramos en revisar conceptos básicos de probabilidad y álgebra lineal, especialmente aquellos relativos a matrices, necesarios para introducir los fundamentos clásicos del aprendizaje. A su vez, estos conocimientos constituyen la base teórica sobre la que también se construyen y formalizan gran parte de los conceptos del aprendizaje moderno.

Posteriormente, nos adentramos en el estudio clásico del \textit{deep learning}, presentando los principales problemas que este paradigma busca resolver y centrándonos especialmente en tareas de regresión y procesamiento de imágenes mediante redes neuronales convolucionales (CNNs). Esta etapa sirve como preámbulo para abordar el dilema clásico del aprendizaje, permitiendo establecer un marco coherente que conecta los fundamentos estadísticos con los desafíos inherentes en este campo.

Una vez definido el concepto de aprendizaje, nos centramos en desarrollar los principales conceptos clásicos asociados, los cuales constituyen su base teórica fundamental. Esta revisión permitió establecer de forma precisa el marco teórico tradicional, sirviendo de punto de partida para comprender las diferencias y desafíos que surgen en el marco del aprendizaje moderno. De igual manera, nos permitió profundizar en la comprensión de conceptos clásicos de generalización, revelando metodologías de utilidad para el estudio de nuestro problema.

El núcleo del proyecto se centra en la exposición del \textit{Deep Double Descent} desde una perspectiva tanto teórica como empírica. Inicialmente, se establece un planteamiento teórico del mismo, resolviendo sus discrepancias con la teoría clásica y realizando un análisis intuitivo de su aparición en problemas de regresión. Posteriormente, se presentan resultados que corroboran su aparición al emplear el descenso de gradiente como método de optimización. 

Por otro lado, se realiza un estudio exhaustivo en la zona de sobreparametrización, donde se obtienen resultados que permiten aclarar su aparición desde un punto de vista teórico, a partir de la definición de sesgos inductivos que guían al modelo hacia soluciones efectivas, fundamentadas en su simplicidad, y que amplían la comprensión más allá de lo que la teoría clásica puede proporcionar, ofreciendo nuevos límites de generalización.

Para finalizar la parte teórica, se introducen conceptos de la aproximación no lineal, estrechamente ligados al concepto del aprendizaje y al propio fenómeno, destacando el uso de distintas bases y diccionarios redundantes de funciones, lo que proporciona una nueva perspectiva para su interpretación.

Para finalizar el proyecto, se lleva a cabo la constatación experimental de los resultados teóricos expuestos. En primer lugar, se comprueba que, al abordar problemas de regresión, la elección de la base utilizada desempeña un papel fundamental en la calidad de las sucesivas aproximaciones. Por otro lado, en tareas de clasificación de imágenes, se observa cómo el fenómeno emerge en contextos típicos del aprendizaje automático, especialmente cuando el modelo supera de largo su capacidad efectiva, ya sea por exceso de parámetros o al realizar el entrenamiento durante un tiempo excesivo. Asimismo, se corrobora que el ruido desempeña un papel crucial tanto en su aparición como en su intensidad.

Por ende, el presente trabajo ha establecido una base sólida para comprender el \textit{Deep Double Descent}. Además, actúa como un puente entre los enfoques clásicos y modernos, ofreciendo una primera aproximación hacia el mundo de la generalización en escenarios de sobreparametrización. No obstante, aún existe un amplio margen para descubrir principios subyacentes a este comportamiento y explorar sus posibles manifestaciones y su relevancia en diversos problemas del mundo actual.

Para el desarrollo de este TFG se han aplicado conocimientos adquiridos en asignaturas como Aprendizaje Automático, Aprendizaje Profundo, Visión por Computador, Probabilidad y Geometría. Además, se han conseguido nuevos conocimientos relacionados con la Aproximación No Lineal, conceptos clásicos y nuevas teorías del Aprendizaje Profundo, así como el uso práctico de herramientas y métodos basados en la biblioteca \textit{PyTorch}.

A modo de conclusión, podemos asegurar que los objetivos presentados al inicio del trabajo se han cumplido de manera satisfactoria, cumpliendo con las expectativas establecidas. No obstante, como se detallará en la siguiente sección, han surgido diversas oportunidades para futuras investigaciones, impulsadas tanto por la naturaleza innovadora de este estudio como por las restricciones de tiempo y recursos computacionales.

\chapter{Trabajos futuros}\label{ch:trabajos-futuros}

Aunque nuestro trabajo ha representado un avance significativo en la comprensión de nuevas dinámicas de actuación de las arquitecturas profundas, especialmente en términos de generalización, aún existen diversos aspectos que requieren un análisis más profundo. Ligado a esto se suma el carácter emergente y en constante evolución de este campo dentro de la IA, que invita a seguir explorando nuevas hipótesis y enfoques que permitan construir un conocimiento más sólido de estos comportamientos.

En primer lugar, una línea de trabajo evidente sería profundizar en experimentos más ambiciosos que no hemos podido llevar a cabo debido a limitaciones en la capacidad de cómputo. Para ello, se propone emplear arquitecturas con una capacidad extremadamente mayor y entrenarlas durante un período de tiempo más prolongado, con el fin de corroborar que, en la mayoría de los casos, se alcanzarán errores inferiores a los del primer descenso. Más aún, sería interesante explorar el uso de otro tipo de arquitecturas de gran escala, tales como los \textit{Transformers} o modelos basados en técnicas de \textit{ensembling}.

Por otra parte, sería interesante indagar en la relación entre el \textit{Deep Double Descent} y el \textit{Grokking}, dado que ambos parecen representar un mismo escenario en términos de generalización en modelos sobreparametrizados. De esta manera, se podría avanzar hacia la unificación de las nuevas tendencias emergentes en el mundo de la generalización dentro de un marco teórico común.

Otra línea de investigación con gran potencial consistiría en llevar a cabo un análisis preliminar del problema a abordar, con el objetivo de estimar de forma anticipada el tiempo y la capacidad computacional necesarios para alcanzar mejoras en el rendimiento durante el segundo descenso. De este modo, se buscaría desarrollar una herramienta capaz de predecir el coste estimado necesario para producir mejoras de rendimiento con respecto a las del mínimo alcanzado durante el descenso clásico inicial. 

Volviendo a los experimentos, sería razonable estudiar el comportamiento del suceso ante datos que incluyan distintos tipos de ruido, no solo en las etiquetas, sino también en las propia entrada. Esto permitiría observar cómo se comporta el suceso frente a distintos escenarios que pueden presentarse en el mundo real, dado que el ruido en estos entornos no sigue, por lo general, una distribución uniforme.

Un campo adicional de exploración consistiría en estudiar el comportamiento del fenómeno en problemas de regresión que utilicen modelos neuronales, con el objetivo de verificar si existen matices o diferencias en comparación con los problemas de clasificación. Además, en estos escenarios, sería interesante explorar un mayor número de bases para la aproximación y estudiar las razones por las cuales ciertas bases pueden presentar el doble descenso mientras que otras no lo hacen.

Como se ha mencionado anteriormente, existe un amplio margen de mejora, especialmente en el ámbito empírico. Sin embargo, el estudio del \textit{Deep Double Descent} ha arrojado luz sobre la explicabilidad de la generalización en la actualidad. Este suceso pone de manifiesto la importancia de no ignorar estas nuevas tendencias, sino de trabajar para conseguir una integración con los enfoques tradicionales, con el objetivo de sentar las bases que permitan unificar los avances recientes con los fundamentos clásicos.

\endinput