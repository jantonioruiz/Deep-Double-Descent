% !TeX root = ../tfg.tex
% !TeX encoding = utf8

\chapter{Trabajos Relacionados}\label{sec:evolucion-ddd}

El \emph{Deep Double Descent}, al estar estrechamente relacionado al avance tecnológico y al crecimiento en la capacidad y el tamaño de los modelos de aprendizaje profundo, ha despertado un mayor interés en los últimos años, como puede observarse en la Figura~\ref{fig:histogram}. No obstante, aunque se percibe durante los últimos años una tendencia al alza del número de artículos que hacen referencia al mismo, únicamente encontramos un total $204$ publicaciones en Scopus\footnote{\label{nota:scopus}Encontradas $204$ publicaciones a fecha $12$ de abril de $2025$ usando la consulta:  
\texttt{TITLE-ABS-KEY ((deep AND double AND descent) OR (overparameterized AND generalization)) AND PUBYEAR < 2025 AND (LIMIT-TO (SUBJAREA,\textquotedblleft COMP\textquotedblright) OR LIMIT-TO (SUBJAREA,\textquotedblleft ENGI\textquotedblright) OR LIMIT-TO (SUBJAREA,\textquotedblleft MATH\textquotedblright) OR LIMIT-TO (SUBJAREA,\textquotedblleft PHYS\textquotedblright)) AND (LIMIT-TO (EXACTKEYWORD,\textquotedblleft Machine Learning\textquotedblright) OR LIMIT-TO (EXACTKEYWORD,\textquotedblleft Deep Learning\textquotedblright) OR LIMIT-TO (EXACTKEYWORD,\textquotedblleft Generalization\textquotedblright) OR LIMIT-TO (EXACTKEYWORD,\textquotedblleft Performance\textquotedblright) OR LIMIT-TO (EXACTKEYWORD,\textquotedblleft Neural-networks\textquotedblright) OR LIMIT-TO (EXACTKEYWORD,\textquotedblleft Deep Neural Networks\textquotedblright) OR LIMIT-TO (EXACTKEYWORD,\textquotedblleft Overparameterization\textquotedblright) OR LIMIT-TO (EXACTKEYWORD,\textquotedblleft Overfitting\textquotedblright) OR LIMIT-TO (EXACTKEYWORD,\textquotedblleft Generalization Error\textquotedblright) OR LIMIT-TO (EXACTKEYWORD,\textquotedblleft Generalization Performance\textquotedblright) OR LIMIT-TO (EXACTKEYWORD,\textquotedblleft Neural Networks\textquotedblright) OR LIMIT-TO (EXACTKEYWORD,\textquotedblleft Interpolation\textquotedblright) OR LIMIT-TO (EXACTKEYWORD,\textquotedblleft Generalize\textquotedblright))}.}. Esta reciente relevancia ilustra el carácter \textbf{innovador} y \textbf{pionero} de este proyecto.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{img/scopus_histogram.png}
    \caption[Número de publicaciones relativas al \textit{Deep Double Descent} en función del año de publicación.] {Número de publicaciones relativas al \textit{Deep Double Descent} en función del año de publicación. Se utilizan las referencias recuperadas desde Scopus\textsuperscript{\ref{nota:scopus}}, complementadas con aquellas que fueron excluidas durante el proceso de filtrado o que no se encuentran disponibles en dicha base de datos.}\label{fig:histogram}
\end{figure}

Sin embargo, aunque la cantidad de artículos científicos aún sea limitada, el interés por parte de investigadores y científicos está creciendo rápidamente. Incluso en ausencia de publicaciones científicas formales, se continúan obteniendo nuevos resultados, tanto teóricos como prácticos, que continúan enriqueciendo nuestra comprensión del problema.

A pesar de la disponibilidad de artículos que abordan el tema, la mayoría de ellos no proporcionan una explicación detallada del mismo, centrándose generalmente en casos prácticos y dejando de lado el análisis teórico intrínseco, limitando su comprensión completa.

\section{Origen y primeras manifestaciones}\label{}

La primera publicación formal relacionada con el \textit{Deep Double Descent} se remonta al año $1989$, como se observa en la Figura~\ref{fig:histogram}. En ese año, Vallet et al.~\cite{Vallet1989} presentaron, de manera empírica, esta dinámica al emplear la pseudoinversa para abordar problemas de regresión lineal con datos artificiales. La siguiente evidencia empírica fue presentada por Krogh \& Hertz en el año $1991$~\cite{Krogh1991}, quienes, de manera parcial, mostraron el suceso utilizando un modelo de regresión lineal.

Posteriormente, nos remontamos hasta $1995$ cuando Opper presenta los primeros resultados teóricos en su artículo ``Statistical Mechanics of Generalization''~\cite{Opper1995}, a través del uso de una red neuronal formada por un perceptrón de una sola capa con una función de activación lineal conocida como ADALINE~\cite{WidrowHoff1988}, y, más tarde, en su revisión del artículo en $2001$ ``Learning to Generalize''~\cite{Opper2001}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{img/estadoarte2.png}
    \caption[\textit{Deep Double Descent} presente en ADALINE.]{Comparación del error de generalización ($\epsilon$) para distintos modelos en función de la fracción entre el número de ejemplos aprendido y el número de parámetros ($\alpha$). Se observa la curva del doble descenso para el modelo ADALINE~\cite{Opper1995}.}\label{fig:estadoarte2}
\end{figure}

En sus representaciones (véase Figura~\ref{fig:estadoarte2}), el error de generalización ($\epsilon$) se muestra en función del número de ejemplos de entrenamiento ($n$) y el número de parámetros ($P$), donde $\alpha = \frac{n}{P}$. Se observa que el error de generalización alcanza su máximo cuando $\alpha$ se aproxima a $1$, es decir, cuando el número de ejemplos de entrenamiento es similar al número de parámetros del modelo. Sumado a ello, demuestra que, para ciertas configuraciones, cuando $N$ tiende a infinito, la solución que proporciona la pseudoinversa mejora a medida que nos alejamos del ``pico'' $(\alpha = 1)$. 

Comportamientos similares a los obtenidos por Opper también han sido reportados por Advani \& Saxe en $2017$~\cite{Advani2017}, así como por Spigler et al.\ y Geiger et al.\ en $2019$~\cite{Spigler2019, Geiger2019}. Estos trabajos, anteriores a la formalización del fenómeno tal como se conoce hoy en día, trabajan con redes neuronales profundas con un gran número de parámetros y estudian el comportamiento del error de generalización utilizando herramientas de física estadística inspiradas en el trabajo de Opper. De esta manera, Spigler et al.\ y Geiger et al.\ proponen una conexión entre el doble descenso y la transición \textit{jamming} propia de la física estadística, mostrando por qué los modelos pueden llegar a generalizar mejor después del ``pico'' del error de test.

No obstante, fue Duin en el año $2000$~\cite{Duin2000} (véanse Figuras 6 y 7) el primero en mostrar curvas de generalización utilizando datos del mundo real, bastante similares a las curvas del doble descenso que tenemos hoy en día.

\section{El nacimiento del \textit{Deep Double Descent}}\label{}

Iniciamos esta sección abordando el equilibrio clásico entre sesgo y varianza, un concepto fundamental en la teoría del aprendizaje automático~\cite{Geman1992, Hastie2001, Bengio2010}. Esta teoría sostiene que, a medida que la complejidad de un modelo aumenta, su sesgo disminuye, pero su varianza se incrementa. Como resultado, llega un punto en el que el error de generalización aumenta, formando la tradicional curva en forma de ``U''. De acuerdo con esta visión tradicional, una vez superado cierto umbral de complejidad, los modelos más grandes son cada vez peores y, por tanto, se busca encontrar un equilibrio en el modelo. Sin embargo, los resultados prácticos modernos no comparten esta teoría. En la actualidad, la visión moderna entre los profesionales es que los \emph{«modelos grandes son mejores»}~\cite{Krizhevsky2012, Neal2019, Huang2019, Szegedy2014}. Estos estudios muestran que el uso de redes neuronales con un gran número de parámetros conduce a un mejor rendimiento, evidenciando que los modelos más complejos pueden obtener resultados superiores a los modelos simples.

Este trabajo se enmarca dentro del estudio de la generalización en redes neuronales profundas, hecho por el que no ha sido percibido con claridad hasta hace relativamente poco tiempo, al estar ligado a una nueva tendencia que favorece el uso de modelos de gran tamaño, impulsada por el incremento de la capacidad de cálculo disponible y la creciente disposición de grandes volúmenes de datos del mundo real. En particular, se enlaza con investigaciones previas, como la de Zhang et al.\ en $2021$~\cite{Zhang2021}, quienes argumentan que comprender el aprendizaje profundo aún requiere replantearse los paradigmas tradicionales de generalización, desafiando la noción clásica de sesgo-varianza.

El \emph{Deep Double Descent} fue nombrado así, por primera vez, por Belkin et al.\ en $2019$ \cite{Belkin2019}, haciendo referencia a los dos descensos que presenta la curva del error de generalización. En este artículo, se busca unificar la teoría clásica del equilibrio sesgo-varianza con los resultados prácticos obtenidos por la teoría moderna, mediante una curva de error unificada (véase~\autoref{fig:estadoarte2.2}). Belkin et al.\ muestran la aparición del doble descenso en diversos modelos y sobre distintos tipos de datos, entre los que se incluyen los árboles de decisión y redes neuronales poco profundas. Además, ofrece una primera intuición sobre su causa argumentando que, en la región sobreparametrizada, el modelo dispone de un mayor número de funciones candidatas compatibles con los datos. A esto se suma el efecto de la regularización implícita inducida por ciertos algoritmos de optimización, como el gradiente descendente~\cite{Soudry2024}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{img/estadoarte2.2.png}
    \caption[Curva del error unificada entre la teoría clásica y moderna~\cite{Belkin2019}.]{Curvas para el error de entrenamiento (línea discontinua) y el error de generalización (línea continua)~\cite{Belkin2019}. Antes del umbral de interpolación, se muestra la curva clásica en ``U''. Después de dicho umbral, se observa la curva del error moderna, induciendo el doble descenso.}\label{fig:estadoarte2.2}
\end{figure}

Posteriormente, Nakkiran et al.\ en $2019$~\cite{Nakkiran2019} observaron que el doble descenso no solo dependía del tamaño del modelo, sino también del número de épocas de entrenamiento. Además, unificaron estos resultados mediante la introducción de una nueva medida de complejidad para un modelo: la \textbf{complejidad efectiva del modelo}, y conjeturaron bajo qué condiciones podía ocurrir en función de dicha medida. En este mismo trabajo, también formalizaron los distintos tipos que pueden manifestarse: en función del número de parámetros, del número de épocas y del tamaño del conjunto de entrenamiento, los cuales constituirán la base conceptual sobre la que desarrollaremos este proyecto.

A modo de reflexión final, es importante destacar que, aunque hoy en día el \textit{Deep Double Descent} pueda parecer muy evidente, durante las últimas décadas la lógica predominante en la comunidad científica iba en otra dirección. En efecto, la mayoría de libros de texto y artículos científicos sobre redes neuronales profundas sostenían que debía controlarse el tamaño de los modelos para evitar el sobreajuste, siguiendo el paradigma clásico del equilibrio sesgo-varianza. Esta idea, que parecía una verdad absoluta e incuestionable, ha sido puesta en duda por estos nuevos comportamientos, motivados estrechamente por la enorme capacidad de cómputo de la que disponemos en la actualidad.

\section{Avances recientes}\label{}

En los últimos años, la comprensión del \textit{Deep Double Descent} ha avanzado significativamente, con nuevas investigaciones que han refinado su caracterización y explorado sus implicaciones en redes neuronales profundas. Uno de los principales avances en el campo del aprendizaje estadístico ha sido la reconsideración de los límites de la sabiduría clásica sobre el sesgo y la varianza, especialmente al analizar el impacto del uso de un gran número de parámetros en el aprendizaje~\cite{Zhang2021,Curth2023}. Por otro lado, Schaeffer et al.\ en $2023$~\cite{Schaeffer2023} realizaron los primeros estudios teóricos y experimentales enfocados en identificar y analizar las posibles causas y factores que pueden desencadenar el fenómeno.

Otras líneas de investigación han explorado cómo ciertas técnicas pueden mitigar su presencia. Por ejemplo, Yang y Suzuki en $2023$~\cite{Yang2024} analizaron el impacto de la regularización mediante el uso de \textit{dropout}, demostrando que dicha técnica puede reducir la magnitud del segundo descenso en el error de generalización. Asimismo, Heckel y Yilmaz en $2021$~\cite{Heckel2020} investigaron cómo el uso de la parada anticipada \emph{o early stopping} puede influir en su aparición.

Desde una perspectiva más empírica, varios estudios han analizado su manifestación en escenarios de aprendizaje adversario. En particular, Min et al. en $2021$~\cite{Ming2020} demostraron que, en algunos casos, un aumento de los datos de entrenamiento puede mejorar la robustez del modelo, aunque también puede provocar un descenso adverso en la generalización.

Singh et al. en $2022$~\cite{Singh2022} presentaron un análisis teórico en redes neuronales de tamaño finito, proporcionando una caracterización matemática básica que ayuda a entender los mecanismos subyacentes. Somepalli et al. en $2022$~\cite{Somepalli2022} investigaron la reproducibilidad del aprendizaje en redes neuronales y su relación con el \textit{Deep Double Descent}.

Por otra parte, estudios modernos han abordado este comportamiento desde la perspectiva de los sesgos inductivos en redes neuronales. En $2025$, Andrew Wilson~\cite{Wilson2025} plantea que la generalización puede entenderse intuitivamente y formalizarse mediante marcos teóricos consolidados, introduciendo los \textit{sesgos inductivos suaves} como principio unificador. Esta visión coincide con la de Mingard et al.\ ($2023$)~\cite{Mingard2023}, quienes argumentan que las redes neuronales siguen un principio similar al de la navaja de Ockham.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/grokking.png}
    \caption[\textit{Grokking} y \textit{Deep Double Descent}.]{Error en entrenamiento y test para el \textit{grokking} (línea verde) y el doble descenso (línea naranja). Se observa que el \textit{grokking} implica una mejora abrupta en términos de generalización una vez superado el umbral de interpolación, mientras que el doble descenso muestra una mejora progresiva a partir de ese punto.}\label{fig:grokking}
\end{figure}

Finalmente, investigaciones recientes han mostrado extensiones del doble descenso, pues este no está necesariamente limitado a dos descensos, sino que, bajo ciertas circunstancias, pueden observarse más de dos descensos~\cite{d_Ascoli2021, Chen2021}. Además, se ha estudiado su relación con otros sucesos emergentes en el aprendizaje profundo, como es el caso del \emph{grokking}~\cite{Power2022}. En este contexto, Davies et al. en $2023$~\cite{Davies2023} propusieron una conexión entre ambos, sugiriendo que el aprendizaje prolongado puede llevar a una mejora abrupta de la generalización (véase Figura~\ref{fig:grokking}). El \textit{grokking}, en particular, suele asociarse a conjuntos de datos pequeños y estructurados, tras un entrenamiento prolongado en el que el modelo transita abruptamente del sobreajuste a la generalización. Por su parte, el \textit{double descent} se manifiesta típicamente en modelos de gran capacidad entrenados con datos ruidosos, en los que se refleja una tendencia decreciente del error vinculada a la capacidad del modelo.

\begin{table}[h]
    \centering
    \small 
    \renewcommand{\arraystretch}{0.9} 
    \begin{NiceTabular}{c c c c}[hvlines,color-inside]
        \Block[fill={cyan!50}]{2-1}{\textbf{Año}} & \Block[fill={cyan!50}]{2-1}{\textbf{Referencia}} & \Block[fill={cyan!50}]{2-1}{\textbf{Principales contribuciones}} & \Block[fill={cyan!50}]{2-1}{\textbf{Citas}} \\ \\
        
        \Block{5-1}{$1989$} & \Block{5-1}{\textit{Linear and nonlinear extension} \\ \textit{of the pseudoinverse solution} \\ \textit{for learning boolean functions.} \\ Vallet et al.~\cite{Vallet1989}} & \Block{5-1}{Primera manifestación empírica sobre \\ datos artificiales en la física teórica.} & \Block{5-1}{$71$} \\ \\ \\ \\ \\

        \Block{4-1}{$1991$} & \Block{4-1}{\textit{A simple weight decay} \\ \textit{can improve generalization.} \\ Krogh \& Hertz~\cite{Krogh1991}} & \Block{4-1}{Evidenciaron, parcialmente, el fenómeno en el \\ contexto de la regresión lineal.} & \Block{4-1}{$2654$} \\ \\ \\ \\ 

        \Block{4-1}{$1995$} & \Block{4-1}{\textit{Statistical mechanics of} \\ \textit{learning: Generalization.} \\ Manfred Opper~\cite{Opper1995}} & \Block{4-1}{Primeros resultados teóricos formales \\ sobre datos artificiales.} & \Block{4-1}{$71$} \\ \\ \\ \\

        \Block{3-1}{$2000$} & \Block{3-1}{\textit{Classifiers in almost empty spaces.} \\ Robert Duin~\cite{Duin2000}} & \Block{3-1}{Primeras evidencias empíricas usando \\ datos del mundo real.} & \Block{3-1}{$209$} \\ \\ \\ 

        \Block{5-1}{$2019$} & \Block{5-1}{\textit{Reconciling modern machine} \\ \textit{learning practice and the} \\ \textit{classical bias-variance trade-off.} \\ Belkin et al.~\cite{Belkin2019}} & \Block{5-1}{Unificaron la sabiduría clásica con los enfoques \\ modernos y acuñaron el nombre de \textit{Deep Double Descent}.} & \Block{5-1}{$2331$} \\ \\ \\ \\ \\

        \Block{4-1}{$2019$} & \Block{4-1}{\textit{Deep double descent: Where bigger} \\ \textit{models and more data hurt.} \\ Nakkiran et al.~\cite{Nakkiran2019}} & \Block{4-1}{Introdujeron la complejidad efectiva del modelo \\ y los distintos tipos de doble descenso.} & \Block{4-1}{$1174$} \\ \\ \\ \\

        \Block{4-1}{$2021$} & \Block{4-1}{\textit{Multiple descent: Design your} \\ \textit{own generalization curve.} \\ Chen et al.~\cite{Chen2021}} & \Block{4-1}{Demostraron que el error puede presentar un \\ número arbitrario de ``picos'' y que pueden controlarse.} & \Block{4-1}{$80$} \\ \\ \\ \\

        \Block{5-1}{$2023$} & \Block{5-1}{\textit{Double descent demystified: Identifying} \\ \textit{interpreting \& ablating the sources} \\ \textit{of a deep learning puzzle.} \\ Schaeffer et al.~\cite{Schaeffer2023}} & \Block{5-1}{Analizaron los factores que provocan su aparición \\ en modelos de regresión polinómica.} & \Block{5-1}{$29$} \\ \\ \\ \\ \\

        \Block{4-1}{$2023$} & \Block{4-1}{\textit{Do deep neural networks have} \\ \textit{an inbuilt occam's razor?} \\ Mingard et al.~\cite{Mingard2023}} & \Block{4-1}{Proponen que las redes neuronales trabajan \\ siguiendo la filosofía de Ockham.} & \Block{4-1}{$13$} \\ \\ \\ \\

        \Block{3-1}{$2023$} & \Block{3-1}{\textit{Unifying grokking and double descent.} \\ Davies et al.~\cite{Davies2023}} & \Block{3-1}{Plantean unificar el \textit{grokking} y el \\ \textit{double descent} bajo un marco conceptual común.} & \Block{3-1}{$37$} \\ \\ \\
    \end{NiceTabular}
    \caption[Resumen de los principales artículos junto con sus contribuciones.]{Resumen de los principales artículos junto con sus contribuciones y citas, extraídas de \textit{Google Scholar}\textsuperscript{\ref{nota:scholar}}.}\label{tabla:resumen-contribuciones}
\end{table}

Para concluir este capítulo, en la Tabla~\ref{tabla:resumen-contribuciones} se presenta un resumen de las principales publicaciones relacionadas con el doble descenso, junto con sus contribuciones más relevantes y el número de citas, obtenidas a partir de Google Scholar\footnote{\label{nota:scholar}Número de citas encontradas a fecha 12 de abril de 2025.}.

\endinput