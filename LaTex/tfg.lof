\babel@toc {spanish}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\babel@toc {spanish}{}\relax 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1}{\ignorespaces Ejemplo de doble descenso profundo en ResNet18~\cite {Nakkiran2019}.}}{\es@scroman {xvi}}{figure.caption.13}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Ejemplos de distribuciones normales.}}{15}{figure.caption.17}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Ejemplos de problemas de clasificación y regresión.}}{23}{figure.caption.18}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Ejemplos de neurona biológica~\cite {Neves2018} y neurona artificial (basada en~\cite {Li2024}).}}{24}{figure.caption.19}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Ejemplo de CNN utilizada para clasificación de imágenes~\cite {CNNSwapna}.}}{26}{figure.caption.20}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Ejemplo de convolución con padding~\cite {Saha2018}.}}{27}{figure.caption.21}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Ejemplos de pooling utilizados en CNN.}}{28}{figure.caption.22}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces Ejemplos de funciones de activación utilizadas en CNN.}}{30}{figure.caption.23}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Diagrama representando el concepto básico de aprendizaje.}}{32}{figure.caption.24}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Distintas tasas de aprendizaje para el descenso de gradiente~\cite {Mostafa2012}.}}{33}{figure.caption.25}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Proceso de retropropagación del error~\cite {Bishop2006}.}}{35}{figure.caption.26}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Distintos casos del conjunto de hipótesis y de la función objetivo.}}{41}{figure.caption.27}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces Ejemplo de curva de aprendizaje tradicional~\cite {Mostafa2012}.}}{42}{figure.caption.28}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces Ejemplos de curvas de aprendizaje modificadas para este proyecto.}}{43}{figure.caption.29}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces Relación bias/variance con underfitting y overfitting en el contexto clásico.}}{46}{figure.caption.30}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Número de publicaciones relativas al \textit {Deep Double Descent} (ir actualizando histograma de cara a nuevos papers).}}{49}{figure.caption.31}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Deep Double Descent presente en ADALINE.}}{50}{figure.caption.32}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Curva del error unificada entre la teoría clásica y moderna~\cite {Belkin2019}.}}{52}{figure.caption.33}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces \textit {Grokking} y \textit {Deep Double Descent}.}}{53}{figure.caption.34}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Ejemplo de doble descenso con las distintas zonas de parametrización.}}{58}{figure.caption.36}%
\contentsline {figure}{\numberline {6.2}{\ignorespaces Curvas típicas del error en función del sesgo y la varianza~\cite {Yang2020}.}}{59}{figure.caption.37}%
\contentsline {figure}{\numberline {6.3}{\ignorespaces Ejemplos de distribuciones de Marchenko-Pastur~\cite {Charles2018}.}}{64}{figure.caption.38}%
\contentsline {figure}{\numberline {6.4}{\ignorespaces Ejemplos de paisajes de la función de pérdida en redes neuronales~\cite {Liu2021}.}}{72}{figure.caption.39}%
\contentsline {figure}{\numberline {6.5}{\ignorespaces Ejemplo de función de pérdida que satisface la condición $\mu $-PL~\cite {Liu2021}.}}{76}{figure.caption.40}%
\contentsline {figure}{\numberline {6.6}{\ignorespaces Distintos modelos polinómicos para aproximar una función.}}{77}{figure.caption.41}%
\contentsline {figure}{\numberline {6.7}{\ignorespaces Fronteras de decisión para modelos con distinta capacidad~\cite {Somepalli2022}.}}{78}{figure.caption.42}%
\contentsline {figure}{\numberline {6.8}{\ignorespaces Diferentes tipos de sesgo para el espacio de hipótesis~\cite {Wilson2025}.}}{79}{figure.caption.43}%
\contentsline {figure}{\numberline {6.9}{\ignorespaces Aumentar la capacidad de un modelo mejora su generalización~\cite {Wilson2025}.}}{80}{figure.caption.44}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {7.1}{\ignorespaces Arquitectura ResNet$18$ modificada.}}{94}{figure.caption.48}%
\contentsline {figure}{\numberline {7.2}{\ignorespaces Arquitectura $3$CNN.}}{95}{figure.caption.50}%
\contentsline {figure}{\numberline {7.3}{\ignorespaces Doble descenso al utilizar aproximación polinómica de Legendre.}}{97}{figure.caption.53}%
\contentsline {figure}{\numberline {7.4}{\ignorespaces Intuición del \textit {Deep Double Descent} usando regresión polinómica.}}{98}{figure.caption.54}%
\contentsline {figure}{\numberline {7.5}{\ignorespaces Normas del vector de parámetros para las aproximaciones polinómicas.}}{99}{figure.caption.55}%
\contentsline {figure}{\numberline {7.6}{\ignorespaces Error en entrenamiento y prueba para la aproximación polinómica.}}{99}{figure.caption.56}%
\contentsline {figure}{\numberline {7.7}{\ignorespaces Aproximaciones polinómicas clásicas de la función objetivo.}}{100}{figure.caption.57}%
\contentsline {figure}{\numberline {7.8}{\ignorespaces Ejemplos de doble descenso en regresión polinómica de Legendre para una función objetivo, tanto con ruido como sin ruido.}}{100}{figure.caption.58}%
\contentsline {figure}{\numberline {7.9}{\ignorespaces Aproximaciones polinómicas de Legendre de la función objetivo, tanto sin ruido como con ruido.}}{101}{figure.caption.59}%
\contentsline {figure}{\numberline {7.10}{\ignorespaces Doble descenso para distintos niveles de ruido.}}{101}{figure.caption.60}%
\contentsline {figure}{\numberline {7.11}{\ignorespaces Umbral de interpolación para el doble descenso con distintos niveles de ruido.}}{102}{figure.caption.61}%
\contentsline {figure}{\numberline {7.12}{\ignorespaces Ejemplo de \textit {sample-wise double descent}.}}{103}{figure.caption.63}%
\contentsline {figure}{\numberline {7.13}{\ignorespaces Ratio parámetros frente a número de ejemplos en el doble descenso.}}{104}{figure.caption.65}%
\contentsline {figure}{\numberline {7.14}{\ignorespaces Doble descenso en función del tamaño del modelo y del número de épocas para la red $2$NN y un subconjunto de MNIST.}}{106}{figure.caption.67}%
\contentsline {figure}{\numberline {7.15}{\ignorespaces Doble descenso en función del tamaño del modelo y del número de épocas para la red $2$NN y un subconjunto de CIFAR-$10$.}}{106}{figure.caption.68}%
\contentsline {figure}{\numberline {7.16}{\ignorespaces Doble descenso en función del tamaño del modelo y del número de épocas para la red $3$CNN y un subconjunto de MNIST.}}{107}{figure.caption.69}%
\contentsline {figure}{\numberline {7.17}{\ignorespaces Doble descenso en función del tamaño del modelo y del número de épocas para la red $3$CNN y un subconjunto de MNIST.}}{107}{figure.caption.70}%
\contentsline {figure}{\numberline {7.18}{\ignorespaces Doble descenso en función del tamaño del modelo y del número de épocas para la red $3$CNN y un subconjunto de CIFAR$10$.}}{108}{figure.caption.71}%
\contentsline {figure}{\numberline {7.19}{\ignorespaces Doble descenso en función del tamaño del modelo y del número de épocas para la red $3$CNN y un subconjunto de CIFAR$100$.}}{108}{figure.caption.72}%
\contentsline {figure}{\numberline {7.20}{\ignorespaces Doble descenso en redes anchas y profundas.}}{109}{figure.caption.73}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {C.1}{\ignorespaces Doble descenso para distintos tamaños de lote.}}{117}{figure.caption.75}%
\contentsline {figure}{\numberline {C.2}{\ignorespaces Doble descenso para distintas tasas de aprendizaje.}}{118}{figure.caption.78}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {D.1}{\ignorespaces Comparativa \textit {epoch-wise double descent} entre un modelo infraparametrizado y uno sobreparametrizado en el que aparecen picos significativos en el error.}}{119}{figure.caption.80}%
\contentsline {figure}{\numberline {D.2}{\ignorespaces Paisaje de la función de pérdida de un modelo sobreparametrizado~\cite {Li2018}.}}{120}{figure.caption.81}%
\contentsline {figure}{\numberline {D.3}{\ignorespaces Paisaje de la función de pérdida de un modelo sobreparametrizado con conexiones residuales~\cite {Li2018}.}}{121}{figure.caption.82}%
\addvspace {10\p@ }
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
