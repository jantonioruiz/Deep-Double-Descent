\babel@toc {spanish}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\babel@toc {spanish}{}\relax 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1}{\ignorespaces Ejemplo de doble descenso profundo en ResNet$18$~\cite {Nakkiran2019}.}}{\es@scroman {xvi}}{figure.caption.13}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Comparación de distribuciones unimodal y multimodal.}}{13}{figure.caption.18}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces Ejemplos de distribuciones normales.}}{14}{figure.caption.19}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Ejemplos de problemas de clasificación y regresión.}}{21}{figure.caption.20}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Ejemplos de neurona biológica~\cite {Neves2018} y neurona artificial (basada en~\cite {Li2024}).}}{22}{figure.caption.21}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Ejemplo de CNN utilizada para clasificación de imágenes~\cite {CNNSwapna}.}}{24}{figure.caption.22}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Ejemplo de convolución con \textit {padding}~\cite {Saha2018}.}}{25}{figure.caption.23}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Ejemplos de \textit {pooling} utilizados en CNN.}}{26}{figure.caption.24}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces Ejemplos de funciones de activación utilizadas en CNN.}}{27}{figure.caption.25}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Diagrama representando el concepto básico de aprendizaje.}}{29}{figure.caption.26}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Distintas tasas de aprendizaje para el descenso de gradiente~\cite {Mostafa2012}.}}{30}{figure.caption.27}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Proceso de retropropagación del error~\cite {Bishop2006}.}}{32}{figure.caption.28}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Distintos casos del conjunto de hipótesis y de la función objetivo.}}{40}{figure.caption.29}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces Ejemplo de curva de aprendizaje tradicional~\cite {Mostafa2012}.}}{41}{figure.caption.30}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces Ejemplos de curvas de aprendizaje modificadas para este proyecto.}}{41}{figure.caption.31}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces Relación \textit {bias/variance} con \textit {underfitting} y \textit {overfitting} en el contexto clásico.}}{44}{figure.caption.32}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces Error en función de la dimensión VC~\cite {Mostafa2012}.}}{47}{figure.caption.33}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Número de publicaciones relativas al \textit {Deep Double Descent} en función del año de publicación.}}{51}{figure.caption.34}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces \textit {Deep Double Descent} presente en ADALINE.}}{52}{figure.caption.35}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Curva del error unificada entre la teoría clásica y moderna~\cite {Belkin2019}.}}{54}{figure.caption.36}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces \textit {Grokking} y \textit {Deep Double Descent}.}}{55}{figure.caption.37}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Ejemplo de doble descenso con las distintas zonas de parametrización.}}{60}{figure.caption.39}%
\contentsline {figure}{\numberline {6.2}{\ignorespaces Curvas típicas del error en función del sesgo y la varianza~\cite {Yang2020}.}}{61}{figure.caption.40}%
\contentsline {figure}{\numberline {6.3}{\ignorespaces Ejemplos de distribuciones de Marchenko-Pastur~\cite {Charles2018}.}}{65}{figure.caption.41}%
\contentsline {figure}{\numberline {6.4}{\ignorespaces Ejemplos de paisajes de la función de pérdida en redes neuronales~\cite {Liu2021}.}}{72}{figure.caption.42}%
\contentsline {figure}{\numberline {6.5}{\ignorespaces Ejemplo de función de pérdida que satisface la condición $\mu $-PL~\cite {Liu2021}.}}{76}{figure.caption.43}%
\contentsline {figure}{\numberline {6.6}{\ignorespaces Distintos modelos polinómicos para aproximar una función.}}{77}{figure.caption.44}%
\contentsline {figure}{\numberline {6.7}{\ignorespaces Fronteras de decisión para modelos con distinta capacidad~\cite {Somepalli2022}.}}{78}{figure.caption.45}%
\contentsline {figure}{\numberline {6.8}{\ignorespaces Diferentes tipos de sesgo para el espacio de hipótesis~\cite {Wilson2025}.}}{79}{figure.caption.46}%
\contentsline {figure}{\numberline {6.9}{\ignorespaces Aumentar la capacidad de un modelo mejora su generalización~\cite {Wilson2025}.}}{80}{figure.caption.47}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {7.1}{\ignorespaces Protocolos de validación experimental.}}{93}{figure.caption.50}%
\contentsline {figure}{\numberline {7.2}{\ignorespaces Arquitectura $2$NN.}}{94}{figure.caption.51}%
\contentsline {figure}{\numberline {7.3}{\ignorespaces Arquitectura ResNet$18$ modificada.}}{95}{figure.caption.53}%
\contentsline {figure}{\numberline {7.4}{\ignorespaces Arquitectura $3$CNN.}}{96}{figure.caption.55}%
\contentsline {figure}{\numberline {7.5}{\ignorespaces Doble descenso al utilizar aproximación polinómica de Legendre y norma del vector de parámetros.}}{100}{figure.caption.59}%
\contentsline {figure}{\numberline {7.6}{\ignorespaces Intuición del \textit {Deep Double Descent} usando regresión polinómica.}}{100}{figure.caption.60}%
\contentsline {figure}{\numberline {7.7}{\ignorespaces Error en entrenamiento y test para las distintas aproximaciones polinómicas.}}{101}{figure.caption.61}%
\contentsline {figure}{\numberline {7.8}{\ignorespaces Normas del vector de parámetros para las aproximaciones polinómicas clásicas.}}{101}{figure.caption.62}%
\contentsline {figure}{\numberline {7.9}{\ignorespaces Aproximaciones polinómicas clásicas de la función objetivo.}}{102}{figure.caption.63}%
\contentsline {figure}{\numberline {7.10}{\ignorespaces Aproximaciones polinómicas de Legendre de la función objetivo, tanto sin ruido como con ruido.}}{103}{figure.caption.64}%
\contentsline {figure}{\numberline {7.11}{\ignorespaces Ejemplos de doble descenso en regresión polinómica de Legendre para una función objetivo, tanto con ruido como sin ruido.}}{104}{figure.caption.65}%
\contentsline {figure}{\numberline {7.12}{\ignorespaces Doble descenso para distintos niveles de ruido.}}{104}{figure.caption.66}%
\contentsline {figure}{\numberline {7.13}{\ignorespaces Umbral de interpolación para el doble descenso con distintos niveles de ruido.}}{105}{figure.caption.67}%
\contentsline {figure}{\numberline {7.14}{\ignorespaces Ejemplo de \textit {sample-wise double descent}.}}{106}{figure.caption.69}%
\contentsline {figure}{\numberline {7.15}{\ignorespaces Ratio parámetros frente a número de ejemplos en el doble descenso.}}{107}{figure.caption.71}%
\contentsline {figure}{\numberline {7.16}{\ignorespaces Doble descenso en función del tamaño del modelo y del número de épocas para la red $2$NN y un subconjunto de MNIST.}}{109}{figure.caption.73}%
\contentsline {figure}{\numberline {7.17}{\ignorespaces Error en entrenamiento y test en función del tamaño del modelo y del número de épocas para la red $2$NN y un subconjunto de CIFAR$10$.}}{110}{figure.caption.74}%
\contentsline {figure}{\numberline {7.18}{\ignorespaces Doble descenso en función del tamaño del modelo y del número de épocas para la red $3$CNN y un subconjunto de MNIST.}}{110}{figure.caption.76}%
\contentsline {figure}{\numberline {7.19}{\ignorespaces Doble descenso en función del tamaño del modelo y del número de épocas para la red $3$CNN y un subconjunto de CIFAR$10$.}}{111}{figure.caption.77}%
\contentsline {figure}{\numberline {7.20}{\ignorespaces Doble descenso en función del tamaño del modelo y del número de épocas para la red $3$CNN y un subconjunto de CIFAR$100$.}}{111}{figure.caption.78}%
\contentsline {figure}{\numberline {7.21}{\ignorespaces Doble descenso en función del tamaño del modelo y del número de épocas para la red ResNet$18$ y el conjunto MNIST.}}{112}{figure.caption.80}%
\contentsline {figure}{\numberline {7.22}{\ignorespaces Doble descenso en función del tamaño del modelo y del número de épocas para la red ResNet$18$ y el conjunto CIFAR$10$.}}{113}{figure.caption.81}%
\contentsline {figure}{\numberline {7.23}{\ignorespaces Doble descenso en función del tamaño del modelo y del número de épocas para la red ResNet$18$ y el conjunto CIFAR$100$.}}{113}{figure.caption.82}%
\contentsline {figure}{\numberline {7.24}{\ignorespaces Doble descenso en función del número de épocas para dos arquitecturas ResNet$18$ sobreparametrizadas y el conjunto CIFAR$10$.}}{114}{figure.caption.83}%
\contentsline {figure}{\numberline {7.25}{\ignorespaces Comparativa del doble descenso entre arquitecturas anchas y profundas.}}{115}{figure.caption.85}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {B.1}{\ignorespaces Doble descenso para distintos tamaños de lote.}}{125}{figure.caption.88}%
\contentsline {figure}{\numberline {B.2}{\ignorespaces Doble descenso para distinto \textit {learning rate}.}}{126}{figure.caption.91}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {C.1}{\ignorespaces Error en entrenamiento y test para las aproximaciones de Legendre y clásica utilizando el GD.}}{127}{figure.caption.93}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {D.1}{\ignorespaces Comparativa \textit {epoch-wise double descent} entre un modelo infraparametrizado y uno sobreparametrizado en el que aparecen picos significativos en el error.}}{128}{figure.caption.94}%
\contentsline {figure}{\numberline {D.2}{\ignorespaces Error en test respecto a distintas configuraciones altamente sobreparametrizadas de la red $2$NN.}}{129}{figure.caption.95}%
\contentsline {figure}{\numberline {D.3}{\ignorespaces Paisajes de la función de pérdida para distintos modelos sobreparametrizados~\cite {Li2018}.}}{130}{figure.caption.96}%
\addvspace {10\p@ }
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
